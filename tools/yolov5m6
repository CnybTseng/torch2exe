digraph {
	graph [size="305.55,305.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1676827789568 [label="
 (1, 255, 88, 160)" fillcolor=darkolivegreen1]
	1676827701088 [label="AddBackward0:1676827701088"]
	1676827700080 -> 1676827701088
	1676827700080 [label="CudnnConvolutionBackward:1676827700080" fillcolor=yellow]
	1676827699240 -> 1676827700080
	1676827699240 [label="SiluBackward:1676827699240" fillcolor=yellow]
	1676827698848 -> 1676827699240
	1676827698848 [label="CudnnBatchNormBackward:1676827698848" fillcolor=yellow]
	1676827698176 -> 1676827698848
	1676827698176 [label="CudnnConvolutionBackward:1676827698176" fillcolor=yellow]
	1676827697616 -> 1676827698176
	1676827697616 [label="CatBackward:1676827697616" fillcolor=yellow]
	1676827635160 -> 1676827697616
	1676827635160 [label="SiluBackward:1676827635160" fillcolor=yellow]
	1676827634768 -> 1676827635160
	1676827634768 [label="CudnnBatchNormBackward:1676827634768" fillcolor=yellow]
	1676827634096 -> 1676827634768
	1676827634096 [label="CudnnConvolutionBackward:1676827634096" fillcolor=yellow]
	1676827633536 -> 1676827634096
	1676827633536 [label="SiluBackward:1676827633536" fillcolor=yellow]
	1676827632472 -> 1676827633536
	1676827632472 [label="CudnnBatchNormBackward:1676827632472" fillcolor=yellow]
	1676827632136 -> 1676827632472
	1676827632136 [label="CudnnConvolutionBackward:1676827632136" fillcolor=yellow]
	1676827565640 -> 1676827632136
	1676827565640 [label="SiluBackward:1676827565640" fillcolor=yellow]
	1676827565248 -> 1676827565640
	1676827565248 [label="CudnnBatchNormBackward:1676827565248" fillcolor=yellow]
	1676827564576 -> 1676827565248
	1676827564576 [label="CudnnConvolutionBackward:1676827564576" fillcolor=yellow]
	1676827564016 -> 1676827564576
	1676827564016 [label="SiluBackward:1676827564016" fillcolor=yellow]
	1676827562840 -> 1676827564016
	1676827562840 [label="CudnnBatchNormBackward:1676827562840" fillcolor=yellow]
	1676827562504 -> 1676827562840
	1676827562504 [label="CudnnConvolutionBackward:1676827562504" fillcolor=yellow]
	1676827495952 -> 1676827562504
	1676827495952 [label="SiluBackward:1676827495952" fillcolor=yellow]
	1676827495560 -> 1676827495952
	1676827495560 [label="CudnnBatchNormBackward:1676827495560" fillcolor=yellow]
	1676827494888 -> 1676827495560
	1676827494888 [label="CudnnConvolutionBackward:1676827494888" fillcolor=yellow]
	1676827494328 -> 1676827494888
	1676827494328 [label="CatBackward:1676827494328" fillcolor=yellow]
	1676827493376 -> 1676827494328
	1676827493376 [label="UpsampleNearest2DBackward1:1676827493376" fillcolor=yellow]
	1676827492984 -> 1676827493376
	1676827492984 [label="SiluBackward:1676827492984" fillcolor=yellow]
	1676827434904 -> 1676827492984
	1676827434904 [label="CudnnBatchNormBackward:1676827434904" fillcolor=yellow]
	1676827434568 -> 1676827434904
	1676827434568 [label="CudnnConvolutionBackward:1676827434568" fillcolor=yellow]
	1676827433448 -> 1676827434568
	1676827433448 [label="SiluBackward:1676827433448" fillcolor=yellow]
	1676827432944 -> 1676827433448
	1676827432944 [label="CudnnBatchNormBackward:1676827432944" fillcolor=yellow]
	1676827432720 -> 1676827432944
	1676827432720 [label="CudnnConvolutionBackward:1676827432720" fillcolor=yellow]
	1676827431712 -> 1676827432720
	1676827431712 [label="CatBackward:1676827431712" fillcolor=yellow]
	1676827430984 -> 1676827431712
	1676827430984 [label="SiluBackward:1676827430984" fillcolor=yellow]
	1676827364880 -> 1676827430984
	1676827364880 [label="CudnnBatchNormBackward:1676827364880" fillcolor=yellow]
	1676827364656 -> 1676827364880
	1676827364656 [label="CudnnConvolutionBackward:1676827364656" fillcolor=yellow]
	1676827363312 -> 1676827364656
	1676827363312 [label="SiluBackward:1676827363312" fillcolor=yellow]
	1676827362528 -> 1676827363312
	1676827362528 [label="CudnnBatchNormBackward:1676827362528" fillcolor=yellow]
	1676827362192 -> 1676827362528
	1676827362192 [label="CudnnConvolutionBackward:1676827362192" fillcolor=yellow]
	1676824678352 -> 1676827362192
	1676824678352 [label="SiluBackward:1676824678352" fillcolor=yellow]
	1676824677848 -> 1676824678352
	1676824677848 [label="CudnnBatchNormBackward:1676824677848" fillcolor=yellow]
	1676824677624 -> 1676824677848
	1676824677624 [label="CudnnConvolutionBackward:1676824677624" fillcolor=yellow]
	1676824676504 -> 1676824677624
	1676824676504 [label="SiluBackward:1676824676504" fillcolor=yellow]
	1676824675776 -> 1676824676504
	1676824675776 [label="CudnnBatchNormBackward:1676824675776" fillcolor=yellow]
	1676824675440 -> 1676824675776
	1676824675440 [label="CudnnConvolutionBackward:1676824675440" fillcolor=yellow]
	1676824612816 -> 1676824675440
	1676824612816 [label="SiluBackward:1676824612816" fillcolor=yellow]
	1676824612312 -> 1676824612816
	1676824612312 [label="CudnnBatchNormBackward:1676824612312" fillcolor=yellow]
	1676824612088 -> 1676824612312
	1676824612088 [label="CudnnConvolutionBackward:1676824612088" fillcolor=yellow]
	1676824611080 -> 1676824612088
	1676824611080 [label="CatBackward:1676824611080" fillcolor=yellow]
	1676824610352 -> 1676824611080
	1676824610352 [label="UpsampleNearest2DBackward1:1676824610352" fillcolor=yellow]
	1676824609848 -> 1676824610352
	1676824609848 [label="SiluBackward:1676824609848" fillcolor=yellow]
	1676824609624 -> 1676824609848
	1676824609624 [label="CudnnBatchNormBackward:1676824609624" fillcolor=yellow]
	1676824538920 -> 1676824609624
	1676824538920 [label="CudnnConvolutionBackward:1676824538920" fillcolor=yellow]
	1676824538360 -> 1676824538920
	1676824538360 [label="SiluBackward:1676824538360" fillcolor=yellow]
	1676824537464 -> 1676824538360
	1676824537464 [label="CudnnBatchNormBackward:1676824537464" fillcolor=yellow]
	1676824537240 -> 1676824537464
	1676824537240 [label="CudnnConvolutionBackward:1676824537240" fillcolor=yellow]
	1676824536232 -> 1676824537240
	1676824536232 [label="CatBackward:1676824536232" fillcolor=yellow]
	1676824535840 -> 1676824536232
	1676824535840 [label="SiluBackward:1676824535840" fillcolor=yellow]
	1676824481576 -> 1676824535840
	1676824481576 [label="CudnnBatchNormBackward:1676824481576" fillcolor=yellow]
	1676824481352 -> 1676824481576
	1676824481352 [label="CudnnConvolutionBackward:1676824481352" fillcolor=yellow]
	1676824480344 -> 1676824481352
	1676824480344 [label="SiluBackward:1676824480344" fillcolor=yellow]
	1676824479952 -> 1676824480344
	1676824479952 [label="CudnnBatchNormBackward:1676824479952" fillcolor=yellow]
	1676824479056 -> 1676824479952
	1676824479056 [label="CudnnConvolutionBackward:1676824479056" fillcolor=yellow]
	1676824478496 -> 1676824479056
	1676824478496 [label="SiluBackward:1676824478496" fillcolor=yellow]
	1676824412056 -> 1676824478496
	1676824412056 [label="CudnnBatchNormBackward:1676824412056" fillcolor=yellow]
	1676824411832 -> 1676824412056
	1676824411832 [label="CudnnConvolutionBackward:1676824411832" fillcolor=yellow]
	1676824410824 -> 1676824411832
	1676824410824 [label="SiluBackward:1676824410824" fillcolor=yellow]
	1676824410432 -> 1676824410824
	1676824410432 [label="CudnnBatchNormBackward:1676824410432" fillcolor=yellow]
	1676824409200 -> 1676824410432
	1676824409200 [label="CudnnConvolutionBackward:1676824409200" fillcolor=yellow]
	1676824408640 -> 1676824409200
	1676824408640 [label="SiluBackward:1676824408640" fillcolor=yellow]
	1676824350336 -> 1676824408640
	1676824350336 [label="CudnnBatchNormBackward:1676824350336" fillcolor=yellow]
	1676824350112 -> 1676824350336
	1676824350112 [label="CudnnConvolutionBackward:1676824350112" fillcolor=yellow]
	1676824349104 -> 1676824350112
	1676824349104 [label="CatBackward:1676824349104" fillcolor=yellow]
	1676824348712 -> 1676824349104
	1676824348712 [label="UpsampleNearest2DBackward1:1676824348712" fillcolor=yellow]
	1676824347760 -> 1676824348712
	1676824347760 [label="SiluBackward:1676824347760" fillcolor=yellow]
	1676824347536 -> 1676824347760
	1676824347536 [label="CudnnBatchNormBackward:1676824347536" fillcolor=yellow]
	1676824346864 -> 1676824347536
	1676824346864 [label="CudnnConvolutionBackward:1676824346864" fillcolor=yellow]
	1676824280704 -> 1676824346864
	1676824280704 [label="SiluBackward:1676824280704" fillcolor=yellow]
	1676824279640 -> 1676824280704
	1676824279640 [label="CudnnBatchNormBackward:1676824279640" fillcolor=yellow]
	1676824279304 -> 1676824279640
	1676824279304 [label="CudnnConvolutionBackward:1676824279304" fillcolor=yellow]
	1676824278408 -> 1676824279304
	1676824278408 [label="CatBackward:1676824278408" fillcolor=yellow]
	1676824278016 -> 1676824278408
	1676824278016 [label="SiluBackward:1676824278016" fillcolor=yellow]
	1676824215336 -> 1676824278016
	1676824215336 [label="CudnnBatchNormBackward:1676824215336" fillcolor=yellow]
	1676824215112 -> 1676824215336
	1676824215112 [label="CudnnConvolutionBackward:1676824215112" fillcolor=yellow]
	1676824213544 -> 1676824215112
	1676824213544 [label="SiluBackward:1676824213544" fillcolor=yellow]
	1676824212760 -> 1676824213544
	1676824212760 [label="CudnnBatchNormBackward:1676824212760" fillcolor=yellow]
	1676824212424 -> 1676824212760
	1676824212424 [label="CudnnConvolutionBackward:1676824212424" fillcolor=yellow]
	1676824211528 -> 1676824212424
	1676824211528 [label="CatBackward:1676824211528" fillcolor=yellow]
	1676824145424 -> 1676824211528
	1676824145424 [label="AddBackward0:1676824145424" fillcolor=yellow]
	1676824144584 -> 1676824145424
	1676824144584 [label="AddBackward0:1676824144584" fillcolor=yellow]
	1676824144080 -> 1676824144584
	1676824144080 [label="SiluBackward:1676824144080" fillcolor=yellow]
	1676824143352 -> 1676824144080
	1676824143352 [label="CudnnBatchNormBackward:1676824143352" fillcolor=yellow]
	1676824143016 -> 1676824143352
	1676824143016 [label="CudnnConvolutionBackward:1676824143016" fillcolor=yellow]
	1676824141896 -> 1676824143016
	1676824141896 [label="SiluBackward:1676824141896" fillcolor=yellow]
	1676824079888 -> 1676824141896
	1676824079888 [label="CudnnBatchNormBackward:1676824079888" fillcolor=yellow]
	1676824079664 -> 1676824079888
	1676824079664 [label="CudnnConvolutionBackward:1676824079664" fillcolor=yellow]
	1676824348096 -> 1676824079664
	1676824348096 [label="SiluBackward:1676824348096" fillcolor=yellow]
	1676824078432 -> 1676824348096
	1676824078432 [label="CudnnBatchNormBackward:1676824078432" fillcolor=yellow]
	1676824077760 -> 1676824078432
	1676824077760 [label="CudnnConvolutionBackward:1676824077760" fillcolor=yellow]
	1676824077200 -> 1676824077760
	1676824077200 [label="CatBackward:1676824077200" fillcolor=yellow]
	1676824018168 -> 1676824077200
	1676824018168 [label="AddBackward0:1676824018168" fillcolor=yellow]
	1676824017776 -> 1676824018168
	1676824017776 [label="AddBackward0:1676824017776" fillcolor=yellow]
	1676824016768 -> 1676824017776
	1676824016768 [label="SiluBackward:1676824016768" fillcolor=yellow]
	1676824016376 -> 1676824016768
	1676824016376 [label="CudnnBatchNormBackward:1676824016376" fillcolor=yellow]
	1676824015704 -> 1676824016376
	1676824015704 [label="CudnnConvolutionBackward:1676824015704" fillcolor=yellow]
	1676824015144 -> 1676824015704
	1676824015144 [label="SiluBackward:1676824015144" fillcolor=yellow]
	1676823948480 -> 1676824015144
	1676823948480 [label="CudnnBatchNormBackward:1676823948480" fillcolor=yellow]
	1676823948256 -> 1676823948480
	1676823948256 [label="CudnnConvolutionBackward:1676823948256" fillcolor=yellow]
	1676824610184 -> 1676823948256
	1676824610184 [label="SiluBackward:1676824610184" fillcolor=yellow]
	1676823947024 -> 1676824610184
	1676823947024 [label="CudnnBatchNormBackward:1676823947024" fillcolor=yellow]
	1676823946240 -> 1676823947024
	1676823946240 [label="CudnnConvolutionBackward:1676823946240" fillcolor=yellow]
	1676823945680 -> 1676823946240
	1676823945680 [label="CatBackward:1676823945680" fillcolor=yellow]
	1676823879240 -> 1676823945680
	1676823879240 [label="AddBackward0:1676823879240" fillcolor=yellow]
	1676823878848 -> 1676823879240
	1676823878848 [label="AddBackward0:1676823878848" fillcolor=yellow]
	1676823877784 -> 1676823878848
	1676823877784 [label="AddBackward0:1676823877784" fillcolor=yellow]
	1676823877392 -> 1676823877784
	1676823877392 [label="AddBackward0:1676823877392" fillcolor=yellow]
	1676823876552 -> 1676823877392
	1676823876552 [label="AddBackward0:1676823876552" fillcolor=yellow]
	1676823876160 -> 1676823876552
	1676823876160 [label="AddBackward0:1676823876160" fillcolor=yellow]
	1676823813816 -> 1676823876160
	1676823813816 [label="SiluBackward:1676823813816" fillcolor=yellow]
	1676823813424 -> 1676823813816
	1676823813424 [label="CudnnBatchNormBackward:1676823813424" fillcolor=yellow]
	1676823812584 -> 1676823813424
	1676823812584 [label="CudnnConvolutionBackward:1676823812584" fillcolor=yellow]
	1676823812024 -> 1676823812584
	1676823812024 [label="SiluBackward:1676823812024" fillcolor=yellow]
	1676823811184 -> 1676823812024
	1676823811184 [label="CudnnBatchNormBackward:1676823811184" fillcolor=yellow]
	1676823810848 -> 1676823811184
	1676823810848 [label="CudnnConvolutionBackward:1676823810848" fillcolor=yellow]
	1676827493208 -> 1676823810848
	1676827493208 [label="SiluBackward:1676827493208" fillcolor=yellow]
	1676823752208 -> 1676827493208
	1676823752208 [label="CudnnBatchNormBackward:1676823752208" fillcolor=yellow]
	1676823751984 -> 1676823752208
	1676823751984 [label="CudnnConvolutionBackward:1676823751984" fillcolor=yellow]
	1676823750864 -> 1676823751984
	1676823750864 [label="CatBackward:1676823750864" fillcolor=yellow]
	1676823750136 -> 1676823750864
	1676823750136 [label="AddBackward0:1676823750136" fillcolor=yellow]
	1676823749632 -> 1676823750136
	1676823749632 [label="AddBackward0:1676823749632" fillcolor=yellow]
	1676823748680 -> 1676823749632
	1676823748680 [label="AddBackward0:1676823748680" fillcolor=yellow]
	1676823678480 -> 1676823748680
	1676823678480 [label="AddBackward0:1676823678480" fillcolor=yellow]
	1676823677752 -> 1676823678480
	1676823677752 [label="SiluBackward:1676823677752" fillcolor=yellow]
	1676823677248 -> 1676823677752
	1676823677248 [label="CudnnBatchNormBackward:1676823677248" fillcolor=yellow]
	1676823677024 -> 1676823677248
	1676823677024 [label="CudnnConvolutionBackward:1676823677024" fillcolor=yellow]
	1676823676016 -> 1676823677024
	1676823676016 [label="SiluBackward:1676823676016" fillcolor=yellow]
	1676823675176 -> 1676823676016
	1676823675176 [label="CudnnBatchNormBackward:1676823675176" fillcolor=yellow]
	1676823617432 -> 1676823675176
	1676823617432 [label="CudnnConvolutionBackward:1676823617432" fillcolor=yellow]
	1676823616536 -> 1676823617432
	1676823616536 [label="SiluBackward:1676823616536" fillcolor=yellow]
	1676823616032 -> 1676823616536
	1676823616032 [label="CudnnBatchNormBackward:1676823616032" fillcolor=yellow]
	1676823615808 -> 1676823616032
	1676823615808 [label="CudnnConvolutionBackward:1676823615808" fillcolor=yellow]
	1676823614800 -> 1676823615808
	1676823614800 [label="CatBackward:1676823614800" fillcolor=yellow]
	1676823613960 -> 1676823614800
	1676823613960 [label="AddBackward0:1676823613960" fillcolor=yellow]
	1676823551952 -> 1676823613960
	1676823551952 [label="AddBackward0:1676823551952" fillcolor=yellow]
	1676823551224 -> 1676823551952
	1676823551224 [label="SiluBackward:1676823551224" fillcolor=yellow]
	1676823550720 -> 1676823551224
	1676823550720 [label="CudnnBatchNormBackward:1676823550720" fillcolor=yellow]
	1676823550496 -> 1676823550720
	1676823550496 [label="CudnnConvolutionBackward:1676823550496" fillcolor=yellow]
	1676823549376 -> 1676823550496
	1676823549376 [label="SiluBackward:1676823549376" fillcolor=yellow]
	1676823548648 -> 1676823549376
	1676823548648 [label="CudnnBatchNormBackward:1676823548648" fillcolor=yellow]
	1676823548312 -> 1676823548648
	1676823548312 [label="CudnnConvolutionBackward:1676823548312" fillcolor=yellow]
	1676823485800 -> 1676823548312
	1676823485800 [label="SiluBackward:1676823485800" fillcolor=yellow]
	1676823485296 -> 1676823485800
	1676823485296 [label="CudnnBatchNormBackward:1676823485296" fillcolor=yellow]
	1676823485072 -> 1676823485296
	1676823485072 [label="CudnnConvolutionBackward:1676823485072" fillcolor=yellow]
	1676823484064 -> 1676823485072
	1676827791080 [label="
 (1, 3, 704, 1280)" fillcolor=lightblue]
	1676827791080 -> 1676823484064
	1676823484064 [label="AccumulateGrad:1676823484064"]
	1676823484008 -> 1676823485072
	1676523400504 [label="model.0.conv.weight
 (48, 3, 6, 6)" fillcolor=lightblue]
	1676523400504 -> 1676823484008
	1676823484008 [label="AccumulateGrad:1676823484008"]
	1676823484568 -> 1676823485296
	1676523400720 [label="model.0.bn.weight
 (48)" fillcolor=lightblue]
	1676523400720 -> 1676823484568
	1676823484568 [label="AccumulateGrad:1676823484568"]
	1676823484400 -> 1676823485296
	1676523400936 [label="model.0.bn.bias
 (48)" fillcolor=lightblue]
	1676523400936 -> 1676823484400
	1676823484400 [label="AccumulateGrad:1676823484400"]
	1676823485632 -> 1676823548312
	1676523463448 [label="model.1.conv.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	1676523463448 -> 1676823485632
	1676823485632 [label="AccumulateGrad:1676823485632"]
	1676823548144 -> 1676823548648
	1676523463808 [label="model.1.bn.weight
 (96)" fillcolor=lightblue]
	1676523463808 -> 1676823548144
	1676823548144 [label="AccumulateGrad:1676823548144"]
	1676823548088 -> 1676823548648
	1676523464024 [label="model.1.bn.bias
 (96)" fillcolor=lightblue]
	1676523464024 -> 1676823548088
	1676823548088 [label="AccumulateGrad:1676823548088"]
	1676823549320 -> 1676823550496
	1676523465032 [label="model.2.cv1.conv.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	1676523465032 -> 1676823549320
	1676823549320 [label="AccumulateGrad:1676823549320"]
	1676823549880 -> 1676823550720
	1676523465392 [label="model.2.cv1.bn.weight
 (48)" fillcolor=lightblue]
	1676523465392 -> 1676823549880
	1676823549880 [label="AccumulateGrad:1676823549880"]
	1676823549712 -> 1676823550720
	1676523465608 [label="model.2.cv1.bn.bias
 (48)" fillcolor=lightblue]
	1676523465608 -> 1676823549712
	1676823549712 [label="AccumulateGrad:1676823549712"]
	1676823551056 -> 1676823551952
	1676823551056 [label="SiluBackward:1676823551056" fillcolor=yellow]
	1676823550664 -> 1676823551056
	1676823550664 [label="CudnnBatchNormBackward:1676823550664" fillcolor=yellow]
	1676524378208 -> 1676823550664
	1676524378208 [label="CudnnConvolutionBackward:1676524378208" fillcolor=yellow]
	1676524376976 -> 1676524378208
	1676524376976 [label="SiluBackward:1676524376976" fillcolor=yellow]
	1676524376584 -> 1676524376976
	1676524376584 [label="CudnnBatchNormBackward:1676524376584" fillcolor=yellow]
	1676524301896 -> 1676524376584
	1676524301896 [label="CudnnConvolutionBackward:1676524301896" fillcolor=yellow]
	1676823551224 -> 1676524301896
	1676524301336 -> 1676524301896
	1676523924216 [label="model.2.m.0.cv1.conv.weight
 (48, 48, 1, 1)" fillcolor=lightblue]
	1676523924216 -> 1676524301336
	1676524301336 [label="AccumulateGrad:1676524301336"]
	1676524301728 -> 1676524376584
	1676523924576 [label="model.2.m.0.cv1.bn.weight
 (48)" fillcolor=lightblue]
	1676523924576 -> 1676524301728
	1676524301728 [label="AccumulateGrad:1676524301728"]
	1676524301560 -> 1676524376584
	1676523924792 [label="model.2.m.0.cv1.bn.bias
 (48)" fillcolor=lightblue]
	1676523924792 -> 1676524301560
	1676524301560 [label="AccumulateGrad:1676524301560"]
	1676524376808 -> 1676524378208
	1676523987160 [label="model.2.m.0.cv2.conv.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	1676523987160 -> 1676524376808
	1676524376808 [label="AccumulateGrad:1676524376808"]
	1676524377816 -> 1676823550664
	1676523987520 [label="model.2.m.0.cv2.bn.weight
 (48)" fillcolor=lightblue]
	1676523987520 -> 1676524377816
	1676524377816 [label="AccumulateGrad:1676524377816"]
	1676524377648 -> 1676823550664
	1676523987736 [label="model.2.m.0.cv2.bn.bias
 (48)" fillcolor=lightblue]
	1676523987736 -> 1676524377648
	1676524377648 [label="AccumulateGrad:1676524377648"]
	1676823551896 -> 1676823613960
	1676823551896 [label="SiluBackward:1676823551896" fillcolor=yellow]
	1676823550888 -> 1676823551896
	1676823550888 [label="CudnnBatchNormBackward:1676823550888" fillcolor=yellow]
	1676524377144 -> 1676823550888
	1676524377144 [label="CudnnConvolutionBackward:1676524377144" fillcolor=yellow]
	1676524248704 -> 1676524377144
	1676524248704 [label="SiluBackward:1676524248704" fillcolor=yellow]
	1676524248312 -> 1676524248704
	1676524248312 [label="CudnnBatchNormBackward:1676524248312" fillcolor=yellow]
	1676524247640 -> 1676524248312
	1676524247640 [label="CudnnConvolutionBackward:1676524247640" fillcolor=yellow]
	1676823551952 -> 1676524247640
	1676524247080 -> 1676524247640
	1676523988672 [label="model.2.m.1.cv1.conv.weight
 (48, 48, 1, 1)" fillcolor=lightblue]
	1676523988672 -> 1676524247080
	1676524247080 [label="AccumulateGrad:1676524247080"]
	1676524247472 -> 1676524248312
	1676523989032 [label="model.2.m.1.cv1.bn.weight
 (48)" fillcolor=lightblue]
	1676523989032 -> 1676524247472
	1676524247472 [label="AccumulateGrad:1676524247472"]
	1676524247304 -> 1676524248312
	1676523989248 [label="model.2.m.1.cv1.bn.bias
 (48)" fillcolor=lightblue]
	1676523989248 -> 1676524247304
	1676524247304 [label="AccumulateGrad:1676524247304"]
	1676524248536 -> 1676524377144
	1676523990112 [label="model.2.m.1.cv2.conv.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	1676523990112 -> 1676524248536
	1676524248536 [label="AccumulateGrad:1676524248536"]
	1676524299096 -> 1676823550888
	1676523990472 [label="model.2.m.1.cv2.bn.weight
 (48)" fillcolor=lightblue]
	1676523990472 -> 1676524299096
	1676524299096 [label="AccumulateGrad:1676524299096"]
	1676524298704 -> 1676823550888
	1676523990688 [label="model.2.m.1.cv2.bn.bias
 (48)" fillcolor=lightblue]
	1676523990688 -> 1676524298704
	1676524298704 [label="AccumulateGrad:1676524298704"]
	1676823613792 -> 1676823614800
	1676823613792 [label="SiluBackward:1676823613792" fillcolor=yellow]
	1676823551728 -> 1676823613792
	1676823551728 [label="CudnnBatchNormBackward:1676823551728" fillcolor=yellow]
	1676524248872 -> 1676823551728
	1676524248872 [label="CudnnConvolutionBackward:1676524248872" fillcolor=yellow]
	1676823549376 -> 1676524248872
	1676524178064 -> 1676524248872
	1676523466472 [label="model.2.cv2.conv.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	1676523466472 -> 1676524178064
	1676524178064 [label="AccumulateGrad:1676524178064"]
	1676524179128 -> 1676823551728
	1676523921552 [label="model.2.cv2.bn.weight
 (48)" fillcolor=lightblue]
	1676523921552 -> 1676524179128
	1676524179128 [label="AccumulateGrad:1676524179128"]
	1676524178736 -> 1676823551728
	1676523921768 [label="model.2.cv2.bn.bias
 (48)" fillcolor=lightblue]
	1676523921768 -> 1676524178736
	1676524178736 [label="AccumulateGrad:1676524178736"]
	1676823614744 -> 1676823615808
	1676523922632 [label="model.2.cv3.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676523922632 -> 1676823614744
	1676823614744 [label="AccumulateGrad:1676823614744"]
	1676823615304 -> 1676823616032
	1676523922992 [label="model.2.cv3.bn.weight
 (96)" fillcolor=lightblue]
	1676523922992 -> 1676823615304
	1676823615304 [label="AccumulateGrad:1676823615304"]
	1676823615136 -> 1676823616032
	1676523923208 [label="model.2.cv3.bn.bias
 (96)" fillcolor=lightblue]
	1676523923208 -> 1676823615136
	1676823615136 [label="AccumulateGrad:1676823615136"]
	1676823616368 -> 1676823617432
	1676524028768 [label="model.3.conv.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	1676524028768 -> 1676823616368
	1676823616368 [label="AccumulateGrad:1676823616368"]
	1676823617264 -> 1676823675176
	1676524029128 [label="model.3.bn.weight
 (192)" fillcolor=lightblue]
	1676524029128 -> 1676823617264
	1676823617264 [label="AccumulateGrad:1676823617264"]
	1676823617208 -> 1676823675176
	1676524029344 [label="model.3.bn.bias
 (192)" fillcolor=lightblue]
	1676524029344 -> 1676823617208
	1676823617208 [label="AccumulateGrad:1676823617208"]
	1676823675960 -> 1676823677024
	1676524030352 [label="model.4.cv1.conv.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	1676524030352 -> 1676823675960
	1676823675960 [label="AccumulateGrad:1676823675960"]
	1676823676520 -> 1676823677248
	1676524030712 [label="model.4.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676524030712 -> 1676823676520
	1676823676520 [label="AccumulateGrad:1676823676520"]
	1676823676352 -> 1676823677248
	1676524030928 [label="model.4.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676524030928 -> 1676823676352
	1676823676352 [label="AccumulateGrad:1676823676352"]
	1676823677584 -> 1676823678480
	1676823677584 [label="SiluBackward:1676823677584" fillcolor=yellow]
	1676823677192 -> 1676823677584
	1676823677192 [label="CudnnBatchNormBackward:1676823677192" fillcolor=yellow]
	1676524108600 -> 1676823677192
	1676524108600 [label="CudnnConvolutionBackward:1676524108600" fillcolor=yellow]
	1676524107368 -> 1676524108600
	1676524107368 [label="SiluBackward:1676524107368" fillcolor=yellow]
	1676524106976 -> 1676524107368
	1676524106976 [label="CudnnBatchNormBackward:1676524106976" fillcolor=yellow]
	1676524106752 -> 1676524106976
	1676524106752 [label="CudnnConvolutionBackward:1676524106752" fillcolor=yellow]
	1676823677752 -> 1676524106752
	1676524039920 -> 1676524106752
	1676524092224 [label="model.4.m.0.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676524092224 -> 1676524039920
	1676524039920 [label="AccumulateGrad:1676524039920"]
	1676524105912 -> 1676524106976
	1676524092584 [label="model.4.m.0.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676524092584 -> 1676524105912
	1676524105912 [label="AccumulateGrad:1676524105912"]
	1676524040144 -> 1676524106976
	1676524092800 [label="model.4.m.0.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676524092800 -> 1676524040144
	1676524040144 [label="AccumulateGrad:1676524040144"]
	1676524107200 -> 1676524108600
	1676524142880 [label="model.4.m.0.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676524142880 -> 1676524107200
	1676524107200 [label="AccumulateGrad:1676524107200"]
	1676524108208 -> 1676823677192
	1676524143240 [label="model.4.m.0.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676524143240 -> 1676524108208
	1676524108208 [label="AccumulateGrad:1676524108208"]
	1676524108152 -> 1676823677192
	1676524143456 [label="model.4.m.0.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676524143456 -> 1676524108152
	1676524108152 [label="AccumulateGrad:1676524108152"]
	1676823678424 -> 1676823748680
	1676823678424 [label="SiluBackward:1676823678424" fillcolor=yellow]
	1676823677416 -> 1676823678424
	1676823677416 [label="CudnnBatchNormBackward:1676823677416" fillcolor=yellow]
	1676524107984 -> 1676823677416
	1676524107984 [label="CudnnConvolutionBackward:1676524107984" fillcolor=yellow]
	1676524036504 -> 1676524107984
	1676524036504 [label="SiluBackward:1676524036504" fillcolor=yellow]
	1676523986896 -> 1676524036504
	1676523986896 [label="CudnnBatchNormBackward:1676523986896" fillcolor=yellow]
	1676523986672 -> 1676523986896
	1676523986672 [label="CudnnConvolutionBackward:1676523986672" fillcolor=yellow]
	1676823678480 -> 1676523986672
	1676523985664 -> 1676523986672
	1676524144464 [label="model.4.m.1.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676524144464 -> 1676523985664
	1676523985664 [label="AccumulateGrad:1676523985664"]
	1676523986056 -> 1676523986896
	1676524144824 [label="model.4.m.1.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676524144824 -> 1676523986056
	1676523986056 [label="AccumulateGrad:1676523986056"]
	1676523985888 -> 1676523986896
	1676524145040 [label="model.4.m.1.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676524145040 -> 1676523985888
	1676523985888 [label="AccumulateGrad:1676523985888"]
	1676524036336 -> 1676524107984
	1676524145904 [label="model.4.m.1.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676524145904 -> 1676524036336
	1676524036336 [label="AccumulateGrad:1676524036336"]
	1676524037680 -> 1676823677416
	1676524146264 [label="model.4.m.1.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676524146264 -> 1676524037680
	1676524037680 [label="AccumulateGrad:1676524037680"]
	1676524037400 -> 1676823677416
	1676524146480 [label="model.4.m.1.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676524146480 -> 1676524037400
	1676524037400 [label="AccumulateGrad:1676524037400"]
	1676823678816 -> 1676823749632
	1676823678816 [label="SiluBackward:1676823678816" fillcolor=yellow]
	1676823678256 -> 1676823678816
	1676823678256 [label="CudnnBatchNormBackward:1676823678256" fillcolor=yellow]
	1676524037232 -> 1676823678256
	1676524037232 [label="CudnnConvolutionBackward:1676524037232" fillcolor=yellow]
	1676523916592 -> 1676524037232
	1676523916592 [label="SiluBackward:1676523916592" fillcolor=yellow]
	1676523916200 -> 1676523916592
	1676523916200 [label="CudnnBatchNormBackward:1676523916200" fillcolor=yellow]
	1676523915976 -> 1676523916200
	1676523915976 [label="CudnnConvolutionBackward:1676523915976" fillcolor=yellow]
	1676823748680 -> 1676523915976
	1676523914744 -> 1676523915976
	1676524204896 [label="model.4.m.2.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676524204896 -> 1676523914744
	1676523914744 [label="AccumulateGrad:1676523914744"]
	1676523915136 -> 1676523916200
	1676524205256 [label="model.4.m.2.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676524205256 -> 1676523915136
	1676523915136 [label="AccumulateGrad:1676523915136"]
	1676523914968 -> 1676523916200
	1676524205472 [label="model.4.m.2.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676524205472 -> 1676523914968
	1676523914968 [label="AccumulateGrad:1676523914968"]
	1676523916424 -> 1676524037232
	1676524206336 [label="model.4.m.2.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676524206336 -> 1676523916424
	1676523916424 [label="AccumulateGrad:1676523916424"]
	1676523983256 -> 1676823678256
	1676524206696 [label="model.4.m.2.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676524206696 -> 1676523983256
	1676523983256 [label="AccumulateGrad:1676523983256"]
	1676523982976 -> 1676823678256
	1676524206912 [label="model.4.m.2.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676524206912 -> 1676523982976
	1676523982976 [label="AccumulateGrad:1676523982976"]
	1676823749576 -> 1676823750136
	1676823749576 [label="SiluBackward:1676823749576" fillcolor=yellow]
	1676823678648 -> 1676823749576
	1676823678648 [label="CudnnBatchNormBackward:1676823678648" fillcolor=yellow]
	1676523917208 -> 1676823678648
	1676523917208 [label="CudnnConvolutionBackward:1676523917208" fillcolor=yellow]
	1676523441008 -> 1676523917208
	1676523441008 [label="SiluBackward:1676523441008" fillcolor=yellow]
	1676523439720 -> 1676523441008
	1676523439720 [label="CudnnBatchNormBackward:1676523439720" fillcolor=yellow]
	1676523396848 -> 1676523439720
	1676523396848 [label="CudnnConvolutionBackward:1676523396848" fillcolor=yellow]
	1676823749632 -> 1676523396848
	1676523330080 -> 1676523396848
	1676524207920 [label="model.4.m.3.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676524207920 -> 1676523330080
	1676523330080 [label="AccumulateGrad:1676523330080"]
	1676523396904 -> 1676523439720
	1676524261592 [label="model.4.m.3.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676524261592 -> 1676523396904
	1676523396904 [label="AccumulateGrad:1676523396904"]
	1676523329744 -> 1676523439720
	1676524261808 [label="model.4.m.3.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676524261808 -> 1676523329744
	1676523329744 [label="AccumulateGrad:1676523329744"]
	1676523440840 -> 1676523917208
	1676524262672 [label="model.4.m.3.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676524262672 -> 1676523440840
	1676523440840 [label="AccumulateGrad:1676523440840"]
	1676523442128 -> 1676823678648
	1676524263032 [label="model.4.m.3.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676524263032 -> 1676523442128
	1676523442128 [label="AccumulateGrad:1676523442128"]
	1676523441120 -> 1676823678648
	1676524263248 [label="model.4.m.3.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676524263248 -> 1676523441120
	1676523441120 [label="AccumulateGrad:1676523441120"]
	1676823749968 -> 1676823750864
	1676823749968 [label="SiluBackward:1676823749968" fillcolor=yellow]
	1676823749408 -> 1676823749968
	1676823749408 [label="CudnnBatchNormBackward:1676823749408" fillcolor=yellow]
	1676523441064 -> 1676823749408
	1676523441064 [label="CudnnConvolutionBackward:1676523441064" fillcolor=yellow]
	1676823676016 -> 1676523441064
	1676827809832 -> 1676523441064
	1676524031792 [label="model.4.cv2.conv.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	1676524031792 -> 1676827809832
	1676827809832 [label="AccumulateGrad:1676827809832"]
	1676827809552 -> 1676823749408
	1676524089560 [label="model.4.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676524089560 -> 1676827809552
	1676827809552 [label="AccumulateGrad:1676827809552"]
	1676827809720 -> 1676823749408
	1676524089776 [label="model.4.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676524089776 -> 1676827809720
	1676827809720 [label="AccumulateGrad:1676827809720"]
	1676823750808 -> 1676823751984
	1676524090640 [label="model.4.cv3.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676524090640 -> 1676823750808
	1676823750808 [label="AccumulateGrad:1676823750808"]
	1676823751368 -> 1676823752208
	1676524091000 [label="model.4.cv3.bn.weight
 (192)" fillcolor=lightblue]
	1676524091000 -> 1676823751368
	1676823751368 [label="AccumulateGrad:1676823751368"]
	1676823751200 -> 1676823752208
	1676524091216 [label="model.4.cv3.bn.bias
 (192)" fillcolor=lightblue]
	1676524091216 -> 1676823751200
	1676823751200 [label="AccumulateGrad:1676823751200"]
	1676823752544 -> 1676823810848
	1676524264400 [label="model.5.conv.weight
 (384, 192, 3, 3)" fillcolor=lightblue]
	1676524264400 -> 1676823752544
	1676823752544 [label="AccumulateGrad:1676823752544"]
	1676823810792 -> 1676823811184
	1676524264760 [label="model.5.bn.weight
 (384)" fillcolor=lightblue]
	1676524264760 -> 1676823810792
	1676823810792 [label="AccumulateGrad:1676823810792"]
	1676823810624 -> 1676823811184
	1676524264976 [label="model.5.bn.bias
 (384)" fillcolor=lightblue]
	1676524264976 -> 1676823810624
	1676823810624 [label="AccumulateGrad:1676823810624"]
	1676823811856 -> 1676823812584
	1676524319296 [label="model.6.cv1.conv.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	1676524319296 -> 1676823811856
	1676823811856 [label="AccumulateGrad:1676823811856"]
	1676823812416 -> 1676823813424
	1676524319656 [label="model.6.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676524319656 -> 1676823812416
	1676823812416 [label="AccumulateGrad:1676823812416"]
	1676823812248 -> 1676823813424
	1676524319872 [label="model.6.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676524319872 -> 1676823812248
	1676823812248 [label="AccumulateGrad:1676823812248"]
	1676823813648 -> 1676823876160
	1676823813648 [label="SiluBackward:1676823813648" fillcolor=yellow]
	1676823813256 -> 1676823813648
	1676823813256 [label="CudnnBatchNormBackward:1676823813256" fillcolor=yellow]
	1676827810784 -> 1676823813256
	1676827810784 [label="CudnnConvolutionBackward:1676827810784" fillcolor=yellow]
	1676827811120 -> 1676827810784
	1676827811120 [label="SiluBackward:1676827811120" fillcolor=yellow]
	1676827811288 -> 1676827811120
	1676827811288 [label="CudnnBatchNormBackward:1676827811288" fillcolor=yellow]
	1676827811400 -> 1676827811288
	1676827811400 [label="CudnnConvolutionBackward:1676827811400" fillcolor=yellow]
	1676823813816 -> 1676827811400
	1676827811624 -> 1676827811400
	1676524360688 [label="model.6.m.0.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676524360688 -> 1676827811624
	1676827811624 [label="AccumulateGrad:1676827811624"]
	1676827811456 -> 1676827811288
	1676524361048 [label="model.6.m.0.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676524361048 -> 1676827811456
	1676827811456 [label="AccumulateGrad:1676827811456"]
	1676827811512 -> 1676827811288
	1676524361264 [label="model.6.m.0.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676524361264 -> 1676827811512
	1676827811512 [label="AccumulateGrad:1676827811512"]
	1676827811176 -> 1676827810784
	1676524362128 [label="model.6.m.0.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676524362128 -> 1676827811176
	1676827811176 [label="AccumulateGrad:1676827811176"]
	1676827810952 -> 1676823813256
	1676524362488 [label="model.6.m.0.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676524362488 -> 1676827810952
	1676827810952 [label="AccumulateGrad:1676827810952"]
	1676827811008 -> 1676823813256
	1676524362704 [label="model.6.m.0.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676524362704 -> 1676827811008
	1676827811008 [label="AccumulateGrad:1676827811008"]
	1676823875992 -> 1676823876552
	1676823875992 [label="SiluBackward:1676823875992" fillcolor=yellow]
	1676823813480 -> 1676823875992
	1676823813480 [label="CudnnBatchNormBackward:1676823813480" fillcolor=yellow]
	1676827811064 -> 1676823813480
	1676827811064 [label="CudnnConvolutionBackward:1676827811064" fillcolor=yellow]
	1676832600752 -> 1676827811064
	1676832600752 [label="SiluBackward:1676832600752" fillcolor=yellow]
	1676832600920 -> 1676832600752
	1676832600920 [label="CudnnBatchNormBackward:1676832600920" fillcolor=yellow]
	1676832601032 -> 1676832600920
	1676832601032 [label="CudnnConvolutionBackward:1676832601032" fillcolor=yellow]
	1676823876160 -> 1676832601032
	1676832601256 -> 1676832601032
	1676524363712 [label="model.6.m.1.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676524363712 -> 1676832601256
	1676832601256 [label="AccumulateGrad:1676832601256"]
	1676832601088 -> 1676832600920
	1676823466344 [label="model.6.m.1.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676823466344 -> 1676832601088
	1676832601088 [label="AccumulateGrad:1676832601088"]
	1676832601144 -> 1676832600920
	1676823466560 [label="model.6.m.1.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676823466560 -> 1676832601144
	1676832601144 [label="AccumulateGrad:1676832601144"]
	1676832600808 -> 1676827811064
	1676823467424 [label="model.6.m.1.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676823467424 -> 1676832600808
	1676832600808 [label="AccumulateGrad:1676832600808"]
	1676832600472 -> 1676823813480
	1676823467784 [label="model.6.m.1.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676823467784 -> 1676832600472
	1676832600472 [label="AccumulateGrad:1676832600472"]
	1676832600640 -> 1676823813480
	1676823468000 [label="model.6.m.1.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676823468000 -> 1676832600640
	1676832600640 [label="AccumulateGrad:1676832600640"]
	1676823876384 -> 1676823877392
	1676823876384 [label="SiluBackward:1676823876384" fillcolor=yellow]
	1676823813984 -> 1676823876384
	1676823813984 [label="CudnnBatchNormBackward:1676823813984" fillcolor=yellow]
	1676832600696 -> 1676823813984
	1676832600696 [label="CudnnConvolutionBackward:1676832600696" fillcolor=yellow]
	1676832602096 -> 1676832600696
	1676832602096 [label="SiluBackward:1676832602096" fillcolor=yellow]
	1676832602264 -> 1676832602096
	1676832602264 [label="CudnnBatchNormBackward:1676832602264" fillcolor=yellow]
	1676832602376 -> 1676832602264
	1676832602376 [label="CudnnConvolutionBackward:1676832602376" fillcolor=yellow]
	1676823876552 -> 1676832602376
	1676832602600 -> 1676832602376
	1676823469008 [label="model.6.m.2.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676823469008 -> 1676832602600
	1676832602600 [label="AccumulateGrad:1676832602600"]
	1676832602432 -> 1676832602264
	1676823469368 [label="model.6.m.2.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676823469368 -> 1676832602432
	1676832602432 [label="AccumulateGrad:1676832602432"]
	1676832602488 -> 1676832602264
	1676823469584 [label="model.6.m.2.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676823469584 -> 1676832602488
	1676832602488 [label="AccumulateGrad:1676832602488"]
	1676832602152 -> 1676832600696
	1676823523760 [label="model.6.m.2.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676823523760 -> 1676832602152
	1676832602152 [label="AccumulateGrad:1676832602152"]
	1676832601816 -> 1676823813984
	1676823524120 [label="model.6.m.2.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676823524120 -> 1676832601816
	1676832601816 [label="AccumulateGrad:1676832601816"]
	1676832601984 -> 1676823813984
	1676823524336 [label="model.6.m.2.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676823524336 -> 1676832601984
	1676832601984 [label="AccumulateGrad:1676832601984"]
	1676823877224 -> 1676823877784
	1676823877224 [label="SiluBackward:1676823877224" fillcolor=yellow]
	1676823876216 -> 1676823877224
	1676823876216 [label="CudnnBatchNormBackward:1676823876216" fillcolor=yellow]
	1676832602040 -> 1676823876216
	1676832602040 [label="CudnnConvolutionBackward:1676832602040" fillcolor=yellow]
	1676832603440 -> 1676832602040
	1676832603440 [label="SiluBackward:1676832603440" fillcolor=yellow]
	1676832603608 -> 1676832603440
	1676832603608 [label="CudnnBatchNormBackward:1676832603608" fillcolor=yellow]
	1676832603720 -> 1676832603608
	1676832603720 [label="CudnnConvolutionBackward:1676832603720" fillcolor=yellow]
	1676823877392 -> 1676832603720
	1676832603944 -> 1676832603720
	1676823525344 [label="model.6.m.3.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676823525344 -> 1676832603944
	1676832603944 [label="AccumulateGrad:1676832603944"]
	1676832603776 -> 1676832603608
	1676823525704 [label="model.6.m.3.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676823525704 -> 1676832603776
	1676832603776 [label="AccumulateGrad:1676832603776"]
	1676832603832 -> 1676832603608
	1676823525920 [label="model.6.m.3.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676823525920 -> 1676832603832
	1676832603832 [label="AccumulateGrad:1676832603832"]
	1676832603496 -> 1676832602040
	1676823526784 [label="model.6.m.3.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676823526784 -> 1676832603496
	1676832603496 [label="AccumulateGrad:1676832603496"]
	1676832603160 -> 1676823876216
	1676823527144 [label="model.6.m.3.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676823527144 -> 1676832603160
	1676832603160 [label="AccumulateGrad:1676832603160"]
	1676832603328 -> 1676823876216
	1676823527360 [label="model.6.m.3.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676823527360 -> 1676832603328
	1676832603328 [label="AccumulateGrad:1676832603328"]
	1676823877616 -> 1676823878848
	1676823877616 [label="SiluBackward:1676823877616" fillcolor=yellow]
	1676823876720 -> 1676823877616
	1676823876720 [label="CudnnBatchNormBackward:1676823876720" fillcolor=yellow]
	1676832603384 -> 1676823876720
	1676832603384 [label="CudnnConvolutionBackward:1676832603384" fillcolor=yellow]
	1676832584368 -> 1676832603384
	1676832584368 [label="SiluBackward:1676832584368" fillcolor=yellow]
	1676832584536 -> 1676832584368
	1676832584536 [label="CudnnBatchNormBackward:1676832584536" fillcolor=yellow]
	1676832584648 -> 1676832584536
	1676832584648 [label="CudnnConvolutionBackward:1676832584648" fillcolor=yellow]
	1676823877784 -> 1676832584648
	1676832584872 -> 1676832584648
	1676823577584 [label="model.6.m.4.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676823577584 -> 1676832584872
	1676832584872 [label="AccumulateGrad:1676832584872"]
	1676832584704 -> 1676832584536
	1676823577944 [label="model.6.m.4.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676823577944 -> 1676832584704
	1676832584704 [label="AccumulateGrad:1676832584704"]
	1676832584760 -> 1676832584536
	1676823578160 [label="model.6.m.4.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676823578160 -> 1676832584760
	1676832584760 [label="AccumulateGrad:1676832584760"]
	1676832584424 -> 1676832603384
	1676823579024 [label="model.6.m.4.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676823579024 -> 1676832584424
	1676832584424 [label="AccumulateGrad:1676832584424"]
	1676832584088 -> 1676823876720
	1676823579384 [label="model.6.m.4.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676823579384 -> 1676832584088
	1676832584088 [label="AccumulateGrad:1676832584088"]
	1676832584256 -> 1676823876720
	1676823579600 [label="model.6.m.4.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676823579600 -> 1676832584256
	1676832584256 [label="AccumulateGrad:1676832584256"]
	1676823878680 -> 1676823879240
	1676823878680 [label="SiluBackward:1676823878680" fillcolor=yellow]
	1676823877448 -> 1676823878680
	1676823877448 [label="CudnnBatchNormBackward:1676823877448" fillcolor=yellow]
	1676832584312 -> 1676823877448
	1676832584312 [label="CudnnConvolutionBackward:1676832584312" fillcolor=yellow]
	1676832585712 -> 1676832584312
	1676832585712 [label="SiluBackward:1676832585712" fillcolor=yellow]
	1676832585880 -> 1676832585712
	1676832585880 [label="CudnnBatchNormBackward:1676832585880" fillcolor=yellow]
	1676832585992 -> 1676832585880
	1676832585992 [label="CudnnConvolutionBackward:1676832585992" fillcolor=yellow]
	1676823878848 -> 1676832585992
	1676832586216 -> 1676832585992
	1676823580608 [label="model.6.m.5.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676823580608 -> 1676832586216
	1676832586216 [label="AccumulateGrad:1676832586216"]
	1676832586048 -> 1676832585880
	1676823638376 [label="model.6.m.5.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676823638376 -> 1676832586048
	1676832586048 [label="AccumulateGrad:1676832586048"]
	1676832586104 -> 1676832585880
	1676823638592 [label="model.6.m.5.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676823638592 -> 1676832586104
	1676832586104 [label="AccumulateGrad:1676832586104"]
	1676832585768 -> 1676832584312
	1676823639456 [label="model.6.m.5.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676823639456 -> 1676832585768
	1676832585768 [label="AccumulateGrad:1676832585768"]
	1676832585432 -> 1676823877448
	1676823639816 [label="model.6.m.5.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676823639816 -> 1676832585432
	1676832585432 [label="AccumulateGrad:1676832585432"]
	1676832585600 -> 1676823877448
	1676823640032 [label="model.6.m.5.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676823640032 -> 1676832585600
	1676832585600 [label="AccumulateGrad:1676832585600"]
	1676823879072 -> 1676823945680
	1676823879072 [label="SiluBackward:1676823879072" fillcolor=yellow]
	1676823877952 -> 1676823879072
	1676823877952 [label="CudnnBatchNormBackward:1676823877952" fillcolor=yellow]
	1676832585656 -> 1676823877952
	1676832585656 [label="CudnnConvolutionBackward:1676832585656" fillcolor=yellow]
	1676823812024 -> 1676832585656
	1676832587056 -> 1676832585656
	1676524320736 [label="model.6.cv2.conv.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	1676524320736 -> 1676832587056
	1676832587056 [label="AccumulateGrad:1676832587056"]
	1676832586776 -> 1676823877952
	1676524321096 [label="model.6.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676524321096 -> 1676832586776
	1676832586776 [label="AccumulateGrad:1676832586776"]
	1676832586944 -> 1676823877952
	1676524321312 [label="model.6.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676524321312 -> 1676832586944
	1676832586944 [label="AccumulateGrad:1676832586944"]
	1676823945512 -> 1676823946240
	1676524322176 [label="model.6.cv3.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676524322176 -> 1676823945512
	1676823945512 [label="AccumulateGrad:1676823945512"]
	1676823946072 -> 1676823947024
	1676524322536 [label="model.6.cv3.bn.weight
 (384)" fillcolor=lightblue]
	1676524322536 -> 1676823946072
	1676823946072 [label="AccumulateGrad:1676823946072"]
	1676823945904 -> 1676823947024
	1676524322752 [label="model.6.cv3.bn.bias
 (384)" fillcolor=lightblue]
	1676524322752 -> 1676823945904
	1676823945904 [label="AccumulateGrad:1676823945904"]
	1676823947248 -> 1676823948256
	1676823641184 [label="model.7.conv.weight
 (576, 384, 3, 3)" fillcolor=lightblue]
	1676823641184 -> 1676823947248
	1676823947248 [label="AccumulateGrad:1676823947248"]
	1676823948088 -> 1676823948480
	1676823641544 [label="model.7.bn.weight
 (576)" fillcolor=lightblue]
	1676823641544 -> 1676823948088
	1676823948088 [label="AccumulateGrad:1676823948088"]
	1676823947584 -> 1676823948480
	1676823641760 [label="model.7.bn.bias
 (576)" fillcolor=lightblue]
	1676823641760 -> 1676823947584
	1676823947584 [label="AccumulateGrad:1676823947584"]
	1676823948816 -> 1676824015704
	1676823691984 [label="model.8.cv1.conv.weight
 (288, 576, 1, 1)" fillcolor=lightblue]
	1676823691984 -> 1676823948816
	1676823948816 [label="AccumulateGrad:1676823948816"]
	1676824015536 -> 1676824016376
	1676823692344 [label="model.8.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676823692344 -> 1676824015536
	1676824015536 [label="AccumulateGrad:1676824015536"]
	1676824015368 -> 1676824016376
	1676823692560 [label="model.8.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676823692560 -> 1676824015368
	1676824015368 [label="AccumulateGrad:1676824015368"]
	1676824016600 -> 1676824017776
	1676824016600 [label="SiluBackward:1676824016600" fillcolor=yellow]
	1676824015872 -> 1676824016600
	1676824015872 [label="CudnnBatchNormBackward:1676824015872" fillcolor=yellow]
	1676832551208 -> 1676824015872
	1676832551208 [label="CudnnConvolutionBackward:1676832551208" fillcolor=yellow]
	1676832551544 -> 1676832551208
	1676832551544 [label="SiluBackward:1676832551544" fillcolor=yellow]
	1676832551712 -> 1676832551544
	1676832551712 [label="CudnnBatchNormBackward:1676832551712" fillcolor=yellow]
	1676832551824 -> 1676832551712
	1676832551824 [label="CudnnConvolutionBackward:1676832551824" fillcolor=yellow]
	1676824016768 -> 1676832551824
	1676832552048 -> 1676832551824
	1676823741568 [label="model.8.m.0.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676823741568 -> 1676832552048
	1676832552048 [label="AccumulateGrad:1676832552048"]
	1676832551880 -> 1676832551712
	1676823741928 [label="model.8.m.0.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676823741928 -> 1676832551880
	1676832551880 [label="AccumulateGrad:1676832551880"]
	1676832551936 -> 1676832551712
	1676823742144 [label="model.8.m.0.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676823742144 -> 1676832551936
	1676832551936 [label="AccumulateGrad:1676832551936"]
	1676832551600 -> 1676832551208
	1676823743008 [label="model.8.m.0.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676823743008 -> 1676832551600
	1676832551600 [label="AccumulateGrad:1676832551600"]
	1676832551376 -> 1676824015872
	1676823743368 [label="model.8.m.0.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676823743368 -> 1676832551376
	1676832551376 [label="AccumulateGrad:1676832551376"]
	1676832551432 -> 1676824015872
	1676823743584 [label="model.8.m.0.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676823743584 -> 1676832551432
	1676832551432 [label="AccumulateGrad:1676832551432"]
	1676824017104 -> 1676824018168
	1676824017104 [label="SiluBackward:1676824017104" fillcolor=yellow]
	1676824016544 -> 1676824017104
	1676824016544 [label="CudnnBatchNormBackward:1676824016544" fillcolor=yellow]
	1676832551488 -> 1676824016544
	1676832551488 [label="CudnnConvolutionBackward:1676832551488" fillcolor=yellow]
	1676832552888 -> 1676832551488
	1676832552888 [label="SiluBackward:1676832552888" fillcolor=yellow]
	1676832553056 -> 1676832552888
	1676832553056 [label="CudnnBatchNormBackward:1676832553056" fillcolor=yellow]
	1676832553168 -> 1676832553056
	1676832553168 [label="CudnnConvolutionBackward:1676832553168" fillcolor=yellow]
	1676824017776 -> 1676832553168
	1676832553392 -> 1676832553168
	1676823802000 [label="model.8.m.1.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676823802000 -> 1676832553392
	1676832553392 [label="AccumulateGrad:1676832553392"]
	1676832553224 -> 1676832553056
	1676823802360 [label="model.8.m.1.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676823802360 -> 1676832553224
	1676832553224 [label="AccumulateGrad:1676832553224"]
	1676832553280 -> 1676832553056
	1676823802576 [label="model.8.m.1.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676823802576 -> 1676832553280
	1676832553280 [label="AccumulateGrad:1676832553280"]
	1676832552944 -> 1676832551488
	1676823803440 [label="model.8.m.1.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676823803440 -> 1676832552944
	1676832552944 [label="AccumulateGrad:1676832552944"]
	1676832552608 -> 1676824016544
	1676823803800 [label="model.8.m.1.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676823803800 -> 1676832552608
	1676832552608 [label="AccumulateGrad:1676832552608"]
	1676832552776 -> 1676824016544
	1676823804016 [label="model.8.m.1.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676823804016 -> 1676832552776
	1676832552776 [label="AccumulateGrad:1676832552776"]
	1676824018000 -> 1676824077200
	1676824018000 [label="SiluBackward:1676824018000" fillcolor=yellow]
	1676824016936 -> 1676824018000
	1676824016936 [label="CudnnBatchNormBackward:1676824016936" fillcolor=yellow]
	1676832552832 -> 1676824016936
	1676832552832 [label="CudnnConvolutionBackward:1676832552832" fillcolor=yellow]
	1676824015144 -> 1676832552832
	1676832554232 -> 1676832552832
	1676823693424 [label="model.8.cv2.conv.weight
 (288, 576, 1, 1)" fillcolor=lightblue]
	1676823693424 -> 1676832554232
	1676832554232 [label="AccumulateGrad:1676832554232"]
	1676832553952 -> 1676824016936
	1676823693784 [label="model.8.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676823693784 -> 1676832553952
	1676832553952 [label="AccumulateGrad:1676832553952"]
	1676832554120 -> 1676824016936
	1676823694000 [label="model.8.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676823694000 -> 1676832554120
	1676832554120 [label="AccumulateGrad:1676832554120"]
	1676824018504 -> 1676824077760
	1676823694864 [label="model.8.cv3.conv.weight
 (576, 576, 1, 1)" fillcolor=lightblue]
	1676823694864 -> 1676824018504
	1676824018504 [label="AccumulateGrad:1676824018504"]
	1676824077592 -> 1676824078432
	1676823695224 [label="model.8.cv3.bn.weight
 (576)" fillcolor=lightblue]
	1676823695224 -> 1676824077592
	1676824077592 [label="AccumulateGrad:1676824077592"]
	1676824077424 -> 1676824078432
	1676823740560 [label="model.8.cv3.bn.bias
 (576)" fillcolor=lightblue]
	1676823740560 -> 1676824077424
	1676824077424 [label="AccumulateGrad:1676824077424"]
	1676824078656 -> 1676824079664
	1676823805168 [label="model.9.conv.weight
 (768, 576, 3, 3)" fillcolor=lightblue]
	1676823805168 -> 1676824078656
	1676824078656 [label="AccumulateGrad:1676824078656"]
	1676824079160 -> 1676824079888
	1676823805528 [label="model.9.bn.weight
 (768)" fillcolor=lightblue]
	1676823805528 -> 1676824079160
	1676824079160 [label="AccumulateGrad:1676824079160"]
	1676824078992 -> 1676824079888
	1676823805744 [label="model.9.bn.bias
 (768)" fillcolor=lightblue]
	1676823805744 -> 1676824078992
	1676824078992 [label="AccumulateGrad:1676824078992"]
	1676824080224 -> 1676824143016
	1676823855968 [label="model.10.cv1.conv.weight
 (384, 768, 1, 1)" fillcolor=lightblue]
	1676823855968 -> 1676824080224
	1676824080224 [label="AccumulateGrad:1676824080224"]
	1676824142848 -> 1676824143352
	1676823856328 [label="model.10.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676823856328 -> 1676824142848
	1676824142848 [label="AccumulateGrad:1676824142848"]
	1676824142792 -> 1676824143352
	1676823856544 [label="model.10.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676823856544 -> 1676824142792
	1676824142792 [label="AccumulateGrad:1676824142792"]
	1676824144024 -> 1676824144584
	1676824144024 [label="SiluBackward:1676824144024" fillcolor=yellow]
	1676824143184 -> 1676824144024
	1676824143184 [label="CudnnBatchNormBackward:1676824143184" fillcolor=yellow]
	1676832563384 -> 1676824143184
	1676832563384 [label="CudnnConvolutionBackward:1676832563384" fillcolor=yellow]
	1676832563776 -> 1676832563384
	1676832563776 [label="SiluBackward:1676832563776" fillcolor=yellow]
	1676832563944 -> 1676832563776
	1676832563944 [label="CudnnBatchNormBackward:1676832563944" fillcolor=yellow]
	1676832564056 -> 1676832563944
	1676832564056 [label="CudnnConvolutionBackward:1676832564056" fillcolor=yellow]
	1676824144080 -> 1676832564056
	1676832564280 -> 1676832564056
	1676823913744 [label="model.10.m.0.cv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676823913744 -> 1676832564280
	1676832564280 [label="AccumulateGrad:1676832564280"]
	1676832564112 -> 1676832563944
	1676823914104 [label="model.10.m.0.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676823914104 -> 1676832564112
	1676832564112 [label="AccumulateGrad:1676832564112"]
	1676832564168 -> 1676832563944
	1676823914320 [label="model.10.m.0.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676823914320 -> 1676832564168
	1676832564168 [label="AccumulateGrad:1676832564168"]
	1676832563832 -> 1676832563384
	1676823915184 [label="model.10.m.0.cv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	1676823915184 -> 1676832563832
	1676832563832 [label="AccumulateGrad:1676832563832"]
	1676832563496 -> 1676824143184
	1676823915544 [label="model.10.m.0.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676823915544 -> 1676832563496
	1676832563496 [label="AccumulateGrad:1676832563496"]
	1676832563664 -> 1676824143184
	1676823915760 [label="model.10.m.0.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676823915760 -> 1676832563664
	1676832563664 [label="AccumulateGrad:1676832563664"]
	1676824144416 -> 1676824145424
	1676824144416 [label="SiluBackward:1676824144416" fillcolor=yellow]
	1676824143856 -> 1676824144416
	1676824143856 [label="CudnnBatchNormBackward:1676824143856" fillcolor=yellow]
	1676832563720 -> 1676824143856
	1676832563720 [label="CudnnConvolutionBackward:1676832563720" fillcolor=yellow]
	1676832565120 -> 1676832563720
	1676832565120 [label="SiluBackward:1676832565120" fillcolor=yellow]
	1676832565288 -> 1676832565120
	1676832565288 [label="CudnnBatchNormBackward:1676832565288" fillcolor=yellow]
	1676832565400 -> 1676832565288
	1676832565400 [label="CudnnConvolutionBackward:1676832565400" fillcolor=yellow]
	1676824144584 -> 1676832565400
	1676832565624 -> 1676832565400
	1676823978272 [label="model.10.m.1.cv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676823978272 -> 1676832565624
	1676832565624 [label="AccumulateGrad:1676832565624"]
	1676832565456 -> 1676832565288
	1676823978632 [label="model.10.m.1.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676823978632 -> 1676832565456
	1676832565456 [label="AccumulateGrad:1676832565456"]
	1676832565512 -> 1676832565288
	1676823978848 [label="model.10.m.1.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676823978848 -> 1676832565512
	1676832565512 [label="AccumulateGrad:1676832565512"]
	1676832565176 -> 1676832563720
	1676823979712 [label="model.10.m.1.cv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	1676823979712 -> 1676832565176
	1676832565176 [label="AccumulateGrad:1676832565176"]
	1676832564672 -> 1676824143856
	1676823980072 [label="model.10.m.1.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676823980072 -> 1676832564672
	1676832564672 [label="AccumulateGrad:1676832564672"]
	1676832564896 -> 1676824143856
	1676823980288 [label="model.10.m.1.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676823980288 -> 1676832564896
	1676832564896 [label="AccumulateGrad:1676832564896"]
	1676824145368 -> 1676824211528
	1676824145368 [label="SiluBackward:1676824145368" fillcolor=yellow]
	1676824144248 -> 1676824145368
	1676824144248 [label="CudnnBatchNormBackward:1676824144248" fillcolor=yellow]
	1676832565064 -> 1676824144248
	1676832565064 [label="CudnnConvolutionBackward:1676832565064" fillcolor=yellow]
	1676824141896 -> 1676832565064
	1676832566464 -> 1676832565064
	1676823857408 [label="model.10.cv2.conv.weight
 (384, 768, 1, 1)" fillcolor=lightblue]
	1676823857408 -> 1676832566464
	1676832566464 [label="AccumulateGrad:1676832566464"]
	1676832566016 -> 1676824144248
	1676823857768 [label="model.10.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676823857768 -> 1676832566016
	1676832566016 [label="AccumulateGrad:1676832566016"]
	1676832566240 -> 1676824144248
	1676823857984 [label="model.10.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676823857984 -> 1676832566240
	1676832566240 [label="AccumulateGrad:1676832566240"]
	1676824145760 -> 1676824212424
	1676823858848 [label="model.10.cv3.conv.weight
 (768, 768, 1, 1)" fillcolor=lightblue]
	1676823858848 -> 1676824145760
	1676824145760 [label="AccumulateGrad:1676824145760"]
	1676824212256 -> 1676824212760
	1676823912520 [label="model.10.cv3.bn.weight
 (768)" fillcolor=lightblue]
	1676823912520 -> 1676824212256
	1676824212256 [label="AccumulateGrad:1676824212256"]
	1676824212200 -> 1676824212760
	1676823912736 [label="model.10.cv3.bn.bias
 (768)" fillcolor=lightblue]
	1676823912736 -> 1676824212200
	1676824212200 [label="AccumulateGrad:1676824212200"]
	1676824213488 -> 1676824215112
	1676823981512 [label="model.11.cv1.conv.weight
 (384, 768, 1, 1)" fillcolor=lightblue]
	1676823981512 -> 1676824213488
	1676824213488 [label="AccumulateGrad:1676824213488"]
	1676824214048 -> 1676824215336
	1676823981872 [label="model.11.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676823981872 -> 1676824214048
	1676824214048 [label="AccumulateGrad:1676824214048"]
	1676824213880 -> 1676824215336
	1676824031304 [label="model.11.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676824031304 -> 1676824213880
	1676824213880 [label="AccumulateGrad:1676824213880"]
	1676824277848 -> 1676824278408
	1676824277848 [label="MaxPool2DWithIndicesBackward:1676824277848" fillcolor=yellow]
	1676824278016 -> 1676824277848
	1676824277344 -> 1676824278408
	1676824277344 [label="MaxPool2DWithIndicesBackward:1676824277344" fillcolor=yellow]
	1676824277848 -> 1676824277344
	1676824277176 -> 1676824278408
	1676824277176 [label="MaxPool2DWithIndicesBackward:1676824277176" fillcolor=yellow]
	1676824277344 -> 1676824277176
	1676824278240 -> 1676824279304
	1676824032168 [label="model.11.cv2.conv.weight
 (768, 1536, 1, 1)" fillcolor=lightblue]
	1676824032168 -> 1676824278240
	1676824278240 [label="AccumulateGrad:1676824278240"]
	1676824279248 -> 1676824279640
	1676824032528 [label="model.11.cv2.bn.weight
 (768)" fillcolor=lightblue]
	1676824032528 -> 1676824279248
	1676824279248 [label="AccumulateGrad:1676824279248"]
	1676824279080 -> 1676824279640
	1676824032744 [label="model.11.cv2.bn.bias
 (768)" fillcolor=lightblue]
	1676824032744 -> 1676824279080
	1676824279080 [label="AccumulateGrad:1676824279080"]
	1676824280536 -> 1676824346864
	1676824033896 [label="model.12.conv.weight
 (576, 768, 1, 1)" fillcolor=lightblue]
	1676824033896 -> 1676824280536
	1676824280536 [label="AccumulateGrad:1676824280536"]
	1676824346696 -> 1676824347536
	1676824034256 [label="model.12.bn.weight
 (576)" fillcolor=lightblue]
	1676824034256 -> 1676824346696
	1676824346696 [label="AccumulateGrad:1676824346696"]
	1676824280928 -> 1676824347536
	1676824034472 [label="model.12.bn.bias
 (576)" fillcolor=lightblue]
	1676824034472 -> 1676824280928
	1676824280928 [label="AccumulateGrad:1676824280928"]
	1676824348096 -> 1676824349104
	1676824348936 -> 1676824350112
	1676824093248 [label="model.15.cv1.conv.weight
 (288, 1152, 1, 1)" fillcolor=lightblue]
	1676824093248 -> 1676824348936
	1676824348936 [label="AccumulateGrad:1676824348936"]
	1676824349944 -> 1676824350336
	1676824093608 [label="model.15.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676824093608 -> 1676824349944
	1676824349944 [label="AccumulateGrad:1676824349944"]
	1676824349440 -> 1676824350336
	1676824093824 [label="model.15.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676824093824 -> 1676824349440
	1676824349440 [label="AccumulateGrad:1676824349440"]
	1676824350672 -> 1676824409200
	1676824146928 [label="model.15.m.0.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676824146928 -> 1676824350672
	1676824350672 [label="AccumulateGrad:1676824350672"]
	1676824409032 -> 1676824410432
	1676824147288 [label="model.15.m.0.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676824147288 -> 1676824409032
	1676824409032 [label="AccumulateGrad:1676824409032"]
	1676824408864 -> 1676824410432
	1676824147504 [label="model.15.m.0.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676824147504 -> 1676824408864
	1676824408864 [label="AccumulateGrad:1676824408864"]
	1676824410656 -> 1676824411832
	1676824148368 [label="model.15.m.0.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676824148368 -> 1676824410656
	1676824410656 [label="AccumulateGrad:1676824410656"]
	1676824411664 -> 1676824412056
	1676824148728 [label="model.15.m.0.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676824148728 -> 1676824411664
	1676824411664 [label="AccumulateGrad:1676824411664"]
	1676824411160 -> 1676824412056
	1676824148944 [label="model.15.m.0.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676824148944 -> 1676824411160
	1676824411160 [label="AccumulateGrad:1676824411160"]
	1676824477992 -> 1676824479056
	1676824149952 [label="model.15.m.1.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676824149952 -> 1676824477992
	1676824477992 [label="AccumulateGrad:1676824477992"]
	1676824478888 -> 1676824479952
	1676824207720 [label="model.15.m.1.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676824207720 -> 1676824478888
	1676824478888 [label="AccumulateGrad:1676824478888"]
	1676824478720 -> 1676824479952
	1676824207936 [label="model.15.m.1.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676824207936 -> 1676824478720
	1676824478720 [label="AccumulateGrad:1676824478720"]
	1676824480176 -> 1676824481352
	1676824208800 [label="model.15.m.1.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676824208800 -> 1676824480176
	1676824480176 [label="AccumulateGrad:1676824480176"]
	1676824481184 -> 1676824481576
	1676824209160 [label="model.15.m.1.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676824209160 -> 1676824481184
	1676824481184 [label="AccumulateGrad:1676824481184"]
	1676824480680 -> 1676824481576
	1676824209376 [label="model.15.m.1.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676824209376 -> 1676824480680
	1676824480680 [label="AccumulateGrad:1676824480680"]
	1676824535224 -> 1676824536232
	1676824535224 [label="SiluBackward:1676824535224" fillcolor=yellow]
	1676824481408 -> 1676824535224
	1676824481408 [label="CudnnBatchNormBackward:1676824481408" fillcolor=yellow]
	1676832630264 -> 1676824481408
	1676832630264 [label="CudnnConvolutionBackward:1676832630264" fillcolor=yellow]
	1676824349104 -> 1676832630264
	1676832630768 -> 1676832630264
	1676824094688 [label="model.15.cv2.conv.weight
 (288, 1152, 1, 1)" fillcolor=lightblue]
	1676824094688 -> 1676832630768
	1676832630768 [label="AccumulateGrad:1676832630768"]
	1676832630488 -> 1676824481408
	1676824095048 [label="model.15.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676824095048 -> 1676832630488
	1676832630488 [label="AccumulateGrad:1676832630488"]
	1676832630656 -> 1676824481408
	1676824095264 [label="model.15.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676824095264 -> 1676832630656
	1676832630656 [label="AccumulateGrad:1676832630656"]
	1676824536064 -> 1676824537240
	1676824096128 [label="model.15.cv3.conv.weight
 (576, 576, 1, 1)" fillcolor=lightblue]
	1676824096128 -> 1676824536064
	1676824536064 [label="AccumulateGrad:1676824536064"]
	1676824537072 -> 1676824537464
	1676824096488 [label="model.15.cv3.bn.weight
 (576)" fillcolor=lightblue]
	1676824096488 -> 1676824537072
	1676824537072 [label="AccumulateGrad:1676824537072"]
	1676824536568 -> 1676824537464
	1676824096704 [label="model.15.cv3.bn.bias
 (576)" fillcolor=lightblue]
	1676824096704 -> 1676824536568
	1676824536568 [label="AccumulateGrad:1676824536568"]
	1676824537800 -> 1676824538920
	1676824210528 [label="model.16.conv.weight
 (384, 576, 1, 1)" fillcolor=lightblue]
	1676824210528 -> 1676824537800
	1676824537800 [label="AccumulateGrad:1676824537800"]
	1676824538752 -> 1676824609624
	1676824210888 [label="model.16.bn.weight
 (384)" fillcolor=lightblue]
	1676824210888 -> 1676824538752
	1676824538752 [label="AccumulateGrad:1676824538752"]
	1676824538584 -> 1676824609624
	1676824211104 [label="model.16.bn.bias
 (384)" fillcolor=lightblue]
	1676824211104 -> 1676824538584
	1676824538584 [label="AccumulateGrad:1676824538584"]
	1676824610184 -> 1676824611080
	1676824611024 -> 1676824612088
	1676824261688 [label="model.19.cv1.conv.weight
 (192, 768, 1, 1)" fillcolor=lightblue]
	1676824261688 -> 1676824611024
	1676824611024 [label="AccumulateGrad:1676824611024"]
	1676824611584 -> 1676824612312
	1676824262048 [label="model.19.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824262048 -> 1676824611584
	1676824611584 [label="AccumulateGrad:1676824611584"]
	1676824611416 -> 1676824612312
	1676824262264 [label="model.19.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824262264 -> 1676824611416
	1676824611416 [label="AccumulateGrad:1676824611416"]
	1676824612648 -> 1676824675440
	1676824323560 [label="model.19.m.0.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676824323560 -> 1676824612648
	1676824612648 [label="AccumulateGrad:1676824612648"]
	1676824675272 -> 1676824675776
	1676824323920 [label="model.19.m.0.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824323920 -> 1676824675272
	1676824675272 [label="AccumulateGrad:1676824675272"]
	1676824675216 -> 1676824675776
	1676824324136 [label="model.19.m.0.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824324136 -> 1676824675216
	1676824675216 [label="AccumulateGrad:1676824675216"]
	1676824676448 -> 1676824677624
	1676824325000 [label="model.19.m.0.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676824325000 -> 1676824676448
	1676824676448 [label="AccumulateGrad:1676824676448"]
	1676824677008 -> 1676824677848
	1676824325360 [label="model.19.m.0.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824325360 -> 1676824677008
	1676824677008 [label="AccumulateGrad:1676824677008"]
	1676824676840 -> 1676824677848
	1676824325576 [label="model.19.m.0.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824325576 -> 1676824676840
	1676824676840 [label="AccumulateGrad:1676824676840"]
	1676824678184 -> 1676827362192
	1676824383992 [label="model.19.m.1.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676824383992 -> 1676824678184
	1676824678184 [label="AccumulateGrad:1676824678184"]
	1676827362024 -> 1676827362528
	1676824384352 [label="model.19.m.1.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824384352 -> 1676827362024
	1676827362024 [label="AccumulateGrad:1676827362024"]
	1676827361968 -> 1676827362528
	1676824384568 [label="model.19.m.1.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824384568 -> 1676827361968
	1676827361968 [label="AccumulateGrad:1676827361968"]
	1676827363256 -> 1676827364656
	1676824385432 [label="model.19.m.1.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676824385432 -> 1676827363256
	1676827363256 [label="AccumulateGrad:1676827363256"]
	1676827363816 -> 1676827364880
	1676824385792 [label="model.19.m.1.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824385792 -> 1676827363816
	1676827363816 [label="AccumulateGrad:1676827363816"]
	1676827363648 -> 1676827364880
	1676824386008 [label="model.19.m.1.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824386008 -> 1676827363648
	1676827363648 [label="AccumulateGrad:1676827363648"]
	1676827365216 -> 1676827431712
	1676827365216 [label="SiluBackward:1676827365216" fillcolor=yellow]
	1676827364824 -> 1676827365216
	1676827364824 [label="CudnnBatchNormBackward:1676827364824" fillcolor=yellow]
	1676832632336 -> 1676827364824
	1676832632336 [label="CudnnConvolutionBackward:1676832632336" fillcolor=yellow]
	1676824611080 -> 1676832632336
	1676832661576 -> 1676832632336
	1676824263128 [label="model.19.cv2.conv.weight
 (192, 768, 1, 1)" fillcolor=lightblue]
	1676824263128 -> 1676832661576
	1676832661576 [label="AccumulateGrad:1676832661576"]
	1676832632560 -> 1676827364824
	1676824263488 [label="model.19.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824263488 -> 1676832632560
	1676832632560 [label="AccumulateGrad:1676832632560"]
	1676832632728 -> 1676827364824
	1676824263704 [label="model.19.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824263704 -> 1676832632728
	1676832632728 [label="AccumulateGrad:1676832632728"]
	1676827431656 -> 1676827432720
	1676824264568 [label="model.19.cv3.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676824264568 -> 1676827431656
	1676827431656 [label="AccumulateGrad:1676827431656"]
	1676827432216 -> 1676827432944
	1676824322336 [label="model.19.cv3.bn.weight
 (384)" fillcolor=lightblue]
	1676824322336 -> 1676827432216
	1676827432216 [label="AccumulateGrad:1676827432216"]
	1676827432048 -> 1676827432944
	1676824322552 [label="model.19.cv3.bn.bias
 (384)" fillcolor=lightblue]
	1676824322552 -> 1676827432048
	1676827432048 [label="AccumulateGrad:1676827432048"]
	1676827433280 -> 1676827434568
	1676824387160 [label="model.20.conv.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	1676824387160 -> 1676827433280
	1676827433280 [label="AccumulateGrad:1676827433280"]
	1676827434400 -> 1676827434904
	1676824387520 [label="model.20.bn.weight
 (192)" fillcolor=lightblue]
	1676824387520 -> 1676827434400
	1676827434400 [label="AccumulateGrad:1676827434400"]
	1676827434344 -> 1676827434904
	1676824432856 [label="model.20.bn.bias
 (192)" fillcolor=lightblue]
	1676824432856 -> 1676827434344
	1676827434344 [label="AccumulateGrad:1676827434344"]
	1676827493208 -> 1676827494328
	1676827494160 -> 1676827494888
	1676824434224 [label="model.23.cv1.conv.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	1676824434224 -> 1676827494160
	1676827494160 [label="AccumulateGrad:1676827494160"]
	1676827494720 -> 1676827495560
	1676824434584 [label="model.23.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676824434584 -> 1676827494720
	1676827494720 [label="AccumulateGrad:1676827494720"]
	1676827494552 -> 1676827495560
	1676824434800 [label="model.23.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676824434800 -> 1676827494552
	1676827494552 [label="AccumulateGrad:1676827494552"]
	1676827495784 -> 1676827562504
	1676824496096 [label="model.23.m.0.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676824496096 -> 1676827495784
	1676827495784 [label="AccumulateGrad:1676827495784"]
	1676827562448 -> 1676827562840
	1676824496456 [label="model.23.m.0.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676824496456 -> 1676827562448
	1676827562448 [label="AccumulateGrad:1676827562448"]
	1676827562280 -> 1676827562840
	1676824496672 [label="model.23.m.0.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676824496672 -> 1676827562280
	1676827562280 [label="AccumulateGrad:1676827562280"]
	1676827563848 -> 1676827564576
	1676824497536 [label="model.23.m.0.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676824497536 -> 1676827563848
	1676827563848 [label="AccumulateGrad:1676827563848"]
	1676827564408 -> 1676827565248
	1676824497896 [label="model.23.m.0.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676824497896 -> 1676827564408
	1676827564408 [label="AccumulateGrad:1676827564408"]
	1676827564240 -> 1676827565248
	1676824498112 [label="model.23.m.0.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676824498112 -> 1676827564240
	1676827564240 [label="AccumulateGrad:1676827564240"]
	1676827565472 -> 1676827632136
	1676824544240 [label="model.23.m.1.cv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	1676824544240 -> 1676827565472
	1676827565472 [label="AccumulateGrad:1676827565472"]
	1676827632080 -> 1676827632472
	1676824544600 [label="model.23.m.1.cv1.bn.weight
 (96)" fillcolor=lightblue]
	1676824544600 -> 1676827632080
	1676827632080 [label="AccumulateGrad:1676827632080"]
	1676827631912 -> 1676827632472
	1676824544816 [label="model.23.m.1.cv1.bn.bias
 (96)" fillcolor=lightblue]
	1676824544816 -> 1676827631912
	1676827631912 [label="AccumulateGrad:1676827631912"]
	1676827633368 -> 1676827634096
	1676824545680 [label="model.23.m.1.cv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	1676824545680 -> 1676827633368
	1676827633368 [label="AccumulateGrad:1676827633368"]
	1676827633928 -> 1676827634768
	1676824546040 [label="model.23.m.1.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676824546040 -> 1676827633928
	1676827633928 [label="AccumulateGrad:1676827633928"]
	1676827633760 -> 1676827634768
	1676824546256 [label="model.23.m.1.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676824546256 -> 1676827633760
	1676827633760 [label="AccumulateGrad:1676827633760"]
	1676827634992 -> 1676827697616
	1676827634992 [label="SiluBackward:1676827634992" fillcolor=yellow]
	1676827634600 -> 1676827634992
	1676827634600 [label="CudnnBatchNormBackward:1676827634600" fillcolor=yellow]
	1676832663144 -> 1676827634600
	1676832663144 [label="CudnnConvolutionBackward:1676832663144" fillcolor=yellow]
	1676827494328 -> 1676832663144
	1676832663648 -> 1676832663144
	1676824435664 [label="model.23.cv2.conv.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	1676824435664 -> 1676832663648
	1676832663648 [label="AccumulateGrad:1676832663648"]
	1676832663368 -> 1676827634600
	1676824436024 [label="model.23.cv2.bn.weight
 (96)" fillcolor=lightblue]
	1676824436024 -> 1676832663368
	1676832663368 [label="AccumulateGrad:1676832663368"]
	1676832663536 -> 1676827634600
	1676824436240 [label="model.23.cv2.bn.bias
 (96)" fillcolor=lightblue]
	1676824436240 -> 1676832663536
	1676832663536 [label="AccumulateGrad:1676832663536"]
	1676827697448 -> 1676827698176
	1676824494512 [label="model.23.cv3.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676824494512 -> 1676827697448
	1676827697448 [label="AccumulateGrad:1676827697448"]
	1676827698008 -> 1676827698848
	1676824494872 [label="model.23.cv3.bn.weight
 (192)" fillcolor=lightblue]
	1676824494872 -> 1676827698008
	1676827698008 [label="AccumulateGrad:1676827698008"]
	1676827697840 -> 1676827698848
	1676824495088 [label="model.23.cv3.bn.bias
 (192)" fillcolor=lightblue]
	1676824495088 -> 1676827697840
	1676827697840 [label="AccumulateGrad:1676827697840"]
	1676827699072 -> 1676827700080
	1676827743792 [label="model.33.m.0.weight
 (255, 192, 1, 1)" fillcolor=lightblue]
	1676827743792 -> 1676827699072
	1676827699072 [label="AccumulateGrad:1676827699072"]
	1676827700024 -> 1676827701088
	1676827700024 [label="ViewBackward:1676827700024"]
	1676827698904 -> 1676827700024
	1676827744008 [label="model.33.m.0.bias
 (255)" fillcolor=lightblue]
	1676827744008 -> 1676827698904
	1676827698904 [label="AccumulateGrad:1676827698904"]
	1676827701088 -> 1676827789568
	1676827790576 [label="
 (1, 255, 44, 80)" fillcolor=darkolivegreen1]
	1676827699408 [label="AddBackward0:1676827699408"]
	1676832664376 -> 1676827699408
	1676832664376 [label="CudnnConvolutionBackward:1676832664376" fillcolor=yellow]
	1676832664600 -> 1676832664376
	1676832664600 [label="SiluBackward:1676832664600" fillcolor=yellow]
	1676832664768 -> 1676832664600
	1676832664768 [label="CudnnBatchNormBackward:1676832664768" fillcolor=yellow]
	1676832664880 -> 1676832664768
	1676832664880 [label="CudnnConvolutionBackward:1676832664880" fillcolor=yellow]
	1676832665104 -> 1676832664880
	1676832665104 [label="CatBackward:1676832665104" fillcolor=yellow]
	1676832665272 -> 1676832665104
	1676832665272 [label="SiluBackward:1676832665272" fillcolor=yellow]
	1676832665440 -> 1676832665272
	1676832665440 [label="CudnnBatchNormBackward:1676832665440" fillcolor=yellow]
	1676832665552 -> 1676832665440
	1676832665552 [label="CudnnConvolutionBackward:1676832665552" fillcolor=yellow]
	1676832686320 -> 1676832665552
	1676832686320 [label="SiluBackward:1676832686320" fillcolor=yellow]
	1676832686488 -> 1676832686320
	1676832686488 [label="CudnnBatchNormBackward:1676832686488" fillcolor=yellow]
	1676832686600 -> 1676832686488
	1676832686600 [label="CudnnConvolutionBackward:1676832686600" fillcolor=yellow]
	1676832686824 -> 1676832686600
	1676832686824 [label="SiluBackward:1676832686824" fillcolor=yellow]
	1676832686992 -> 1676832686824
	1676832686992 [label="CudnnBatchNormBackward:1676832686992" fillcolor=yellow]
	1676832687104 -> 1676832686992
	1676832687104 [label="CudnnConvolutionBackward:1676832687104" fillcolor=yellow]
	1676832687328 -> 1676832687104
	1676832687328 [label="SiluBackward:1676832687328" fillcolor=yellow]
	1676832687496 -> 1676832687328
	1676832687496 [label="CudnnBatchNormBackward:1676832687496" fillcolor=yellow]
	1676832687608 -> 1676832687496
	1676832687608 [label="CudnnConvolutionBackward:1676832687608" fillcolor=yellow]
	1676832687832 -> 1676832687608
	1676832687832 [label="SiluBackward:1676832687832" fillcolor=yellow]
	1676832688000 -> 1676832687832
	1676832688000 [label="CudnnBatchNormBackward:1676832688000" fillcolor=yellow]
	1676832688112 -> 1676832688000
	1676832688112 [label="CudnnConvolutionBackward:1676832688112" fillcolor=yellow]
	1676832688336 -> 1676832688112
	1676832688336 [label="CatBackward:1676832688336" fillcolor=yellow]
	1676832688504 -> 1676832688336
	1676832688504 [label="SiluBackward:1676832688504" fillcolor=yellow]
	1676832688616 -> 1676832688504
	1676832688616 [label="CudnnBatchNormBackward:1676832688616" fillcolor=yellow]
	1676832688728 -> 1676832688616
	1676832688728 [label="CudnnConvolutionBackward:1676832688728" fillcolor=yellow]
	1676827699240 -> 1676832688728
	1676832688952 -> 1676832688728
	1676824596624 [label="model.24.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676824596624 -> 1676832688952
	1676832688952 [label="AccumulateGrad:1676832688952"]
	1676832688784 -> 1676832688616
	1676824596984 [label="model.24.bn.weight
 (192)" fillcolor=lightblue]
	1676824596984 -> 1676832688784
	1676832688784 [label="AccumulateGrad:1676832688784"]
	1676832688840 -> 1676832688616
	1676824597200 [label="model.24.bn.bias
 (192)" fillcolor=lightblue]
	1676824597200 -> 1676832688840
	1676832688840 [label="AccumulateGrad:1676832688840"]
	1676827492984 -> 1676832688336
	1676832688392 -> 1676832688112
	1676824598424 [label="model.26.cv1.conv.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	1676824598424 -> 1676832688392
	1676832688392 [label="AccumulateGrad:1676832688392"]
	1676832688168 -> 1676832688000
	1676824598784 [label="model.26.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824598784 -> 1676832688168
	1676832688168 [label="AccumulateGrad:1676832688168"]
	1676832688224 -> 1676832688000
	1676824599000 [label="model.26.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824599000 -> 1676832688224
	1676832688224 [label="AccumulateGrad:1676832688224"]
	1676832687888 -> 1676832687608
	1676824656200 [label="model.26.m.0.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676824656200 -> 1676832687888
	1676832687888 [label="AccumulateGrad:1676832687888"]
	1676832687664 -> 1676832687496
	1676824656560 [label="model.26.m.0.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824656560 -> 1676832687664
	1676832687664 [label="AccumulateGrad:1676832687664"]
	1676832687720 -> 1676832687496
	1676824656776 [label="model.26.m.0.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824656776 -> 1676832687720
	1676832687720 [label="AccumulateGrad:1676832687720"]
	1676832687384 -> 1676832687104
	1676824657640 [label="model.26.m.0.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676824657640 -> 1676832687384
	1676832687384 [label="AccumulateGrad:1676832687384"]
	1676832687160 -> 1676832686992
	1676824715408 [label="model.26.m.0.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824715408 -> 1676832687160
	1676832687160 [label="AccumulateGrad:1676832687160"]
	1676832687216 -> 1676832686992
	1676824715624 [label="model.26.m.0.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824715624 -> 1676832687216
	1676832687216 [label="AccumulateGrad:1676832687216"]
	1676832686880 -> 1676832686600
	1676824716632 [label="model.26.m.1.cv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	1676824716632 -> 1676832686880
	1676832686880 [label="AccumulateGrad:1676832686880"]
	1676832686656 -> 1676832686488
	1676824716992 [label="model.26.m.1.cv1.bn.weight
 (192)" fillcolor=lightblue]
	1676824716992 -> 1676832686656
	1676832686656 [label="AccumulateGrad:1676832686656"]
	1676832686712 -> 1676832686488
	1676824717208 [label="model.26.m.1.cv1.bn.bias
 (192)" fillcolor=lightblue]
	1676824717208 -> 1676832686712
	1676832686712 [label="AccumulateGrad:1676832686712"]
	1676832686376 -> 1676832665552
	1676824718072 [label="model.26.m.1.cv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	1676824718072 -> 1676832686376
	1676832686376 [label="AccumulateGrad:1676832686376"]
	1676832686152 -> 1676832665440
	1676824718432 [label="model.26.m.1.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824718432 -> 1676832686152
	1676832686152 [label="AccumulateGrad:1676832686152"]
	1676832686208 -> 1676832665440
	1676824718648 [label="model.26.m.1.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824718648 -> 1676832686208
	1676832686208 [label="AccumulateGrad:1676832686208"]
	1676832665328 -> 1676832665104
	1676832665328 [label="SiluBackward:1676832665328" fillcolor=yellow]
	1676832665496 -> 1676832665328
	1676832665496 [label="CudnnBatchNormBackward:1676832665496" fillcolor=yellow]
	1676832698440 -> 1676832665496
	1676832698440 [label="CudnnConvolutionBackward:1676832698440" fillcolor=yellow]
	1676832688336 -> 1676832698440
	1676832698944 -> 1676832698440
	1676824599864 [label="model.26.cv2.conv.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	1676824599864 -> 1676832698944
	1676832698944 [label="AccumulateGrad:1676832698944"]
	1676832698664 -> 1676832665496
	1676824600224 [label="model.26.cv2.bn.weight
 (192)" fillcolor=lightblue]
	1676824600224 -> 1676832698664
	1676832698664 [label="AccumulateGrad:1676832698664"]
	1676832698832 -> 1676832665496
	1676824600440 [label="model.26.cv2.bn.bias
 (192)" fillcolor=lightblue]
	1676824600440 -> 1676832698832
	1676832698832 [label="AccumulateGrad:1676832698832"]
	1676832665160 -> 1676832664880
	1676824654616 [label="model.26.cv3.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676824654616 -> 1676832665160
	1676832665160 [label="AccumulateGrad:1676832665160"]
	1676832664936 -> 1676832664768
	1676824654976 [label="model.26.cv3.bn.weight
 (384)" fillcolor=lightblue]
	1676824654976 -> 1676832664936
	1676832664936 [label="AccumulateGrad:1676832664936"]
	1676832664992 -> 1676832664768
	1676824655192 [label="model.26.cv3.bn.bias
 (384)" fillcolor=lightblue]
	1676824655192 -> 1676832664992
	1676832664992 [label="AccumulateGrad:1676832664992"]
	1676832664656 -> 1676832664376
	1676827744368 [label="model.33.m.1.weight
 (255, 384, 1, 1)" fillcolor=lightblue]
	1676827744368 -> 1676832664656
	1676832664656 [label="AccumulateGrad:1676832664656"]
	1676832664432 -> 1676827699408
	1676832664432 [label="ViewBackward:1676832664432"]
	1676832664712 -> 1676832664432
	1676827744584 [label="model.33.m.1.bias
 (255)" fillcolor=lightblue]
	1676827744584 -> 1676832664712
	1676832664712 [label="AccumulateGrad:1676832664712"]
	1676827699408 -> 1676827790576
	1676827789784 [label="
 (1, 255, 22, 40)" fillcolor=darkolivegreen1]
	1676832664544 [label="AddBackward0:1676832664544"]
	1676832699672 -> 1676832664544
	1676832699672 [label="CudnnConvolutionBackward:1676832699672" fillcolor=yellow]
	1676832699896 -> 1676832699672
	1676832699896 [label="SiluBackward:1676832699896" fillcolor=yellow]
	1676832700064 -> 1676832699896
	1676832700064 [label="CudnnBatchNormBackward:1676832700064" fillcolor=yellow]
	1676832700176 -> 1676832700064
	1676832700176 [label="CudnnConvolutionBackward:1676832700176" fillcolor=yellow]
	1676832700400 -> 1676832700176
	1676832700400 [label="CatBackward:1676832700400" fillcolor=yellow]
	1676832700568 -> 1676832700400
	1676832700568 [label="SiluBackward:1676832700568" fillcolor=yellow]
	1676832700736 -> 1676832700568
	1676832700736 [label="CudnnBatchNormBackward:1676832700736" fillcolor=yellow]
	1676832700848 -> 1676832700736
	1676832700848 [label="CudnnConvolutionBackward:1676832700848" fillcolor=yellow]
	1676832701072 -> 1676832700848
	1676832701072 [label="SiluBackward:1676832701072" fillcolor=yellow]
	1676832701240 -> 1676832701072
	1676832701240 [label="CudnnBatchNormBackward:1676832701240" fillcolor=yellow]
	1676832701352 -> 1676832701240
	1676832701352 [label="CudnnConvolutionBackward:1676832701352" fillcolor=yellow]
	1676832701576 -> 1676832701352
	1676832701576 [label="SiluBackward:1676832701576" fillcolor=yellow]
	1676832701744 -> 1676832701576
	1676832701744 [label="CudnnBatchNormBackward:1676832701744" fillcolor=yellow]
	1676832701856 -> 1676832701744
	1676832701856 [label="CudnnConvolutionBackward:1676832701856" fillcolor=yellow]
	1676832702080 -> 1676832701856
	1676832702080 [label="SiluBackward:1676832702080" fillcolor=yellow]
	1676832702248 -> 1676832702080
	1676832702248 [label="CudnnBatchNormBackward:1676832702248" fillcolor=yellow]
	1676832702360 -> 1676832702248
	1676832702360 [label="CudnnConvolutionBackward:1676832702360" fillcolor=yellow]
	1676832723128 -> 1676832702360
	1676832723128 [label="SiluBackward:1676832723128" fillcolor=yellow]
	1676832723296 -> 1676832723128
	1676832723296 [label="CudnnBatchNormBackward:1676832723296" fillcolor=yellow]
	1676832723408 -> 1676832723296
	1676832723408 [label="CudnnConvolutionBackward:1676832723408" fillcolor=yellow]
	1676832723632 -> 1676832723408
	1676832723632 [label="CatBackward:1676832723632" fillcolor=yellow]
	1676832723800 -> 1676832723632
	1676832723800 [label="SiluBackward:1676832723800" fillcolor=yellow]
	1676832723912 -> 1676832723800
	1676832723912 [label="CudnnBatchNormBackward:1676832723912" fillcolor=yellow]
	1676832724024 -> 1676832723912
	1676832724024 [label="CudnnConvolutionBackward:1676832724024" fillcolor=yellow]
	1676832664600 -> 1676832724024
	1676832724248 -> 1676832724024
	1676827398648 [label="model.27.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	1676827398648 -> 1676832724248
	1676832724248 [label="AccumulateGrad:1676832724248"]
	1676832724080 -> 1676832723912
	1676827399008 [label="model.27.bn.weight
 (384)" fillcolor=lightblue]
	1676827399008 -> 1676832724080
	1676832724080 [label="AccumulateGrad:1676832724080"]
	1676832724136 -> 1676832723912
	1676827399224 [label="model.27.bn.bias
 (384)" fillcolor=lightblue]
	1676827399224 -> 1676832724136
	1676832724136 [label="AccumulateGrad:1676832724136"]
	1676824609848 -> 1676832723632
	1676832723688 -> 1676832723408
	1676827400448 [label="model.29.cv1.conv.weight
 (288, 768, 1, 1)" fillcolor=lightblue]
	1676827400448 -> 1676832723688
	1676832723688 [label="AccumulateGrad:1676832723688"]
	1676832723464 -> 1676832723296
	1676827400808 [label="model.29.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676827400808 -> 1676832723464
	1676832723464 [label="AccumulateGrad:1676832723464"]
	1676832723520 -> 1676832723296
	1676827401024 [label="model.29.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676827401024 -> 1676832723520
	1676832723520 [label="AccumulateGrad:1676832723520"]
	1676832723184 -> 1676832702360
	1676827454128 [label="model.29.m.0.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676827454128 -> 1676832723184
	1676832723184 [label="AccumulateGrad:1676832723184"]
	1676832702416 -> 1676832702248
	1676827454488 [label="model.29.m.0.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676827454488 -> 1676832702416
	1676832702416 [label="AccumulateGrad:1676832702416"]
	1676832723016 -> 1676832702248
	1676827454704 [label="model.29.m.0.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676827454704 -> 1676832723016
	1676832723016 [label="AccumulateGrad:1676832723016"]
	1676832702136 -> 1676832701856
	1676827508880 [label="model.29.m.0.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676827508880 -> 1676832702136
	1676832702136 [label="AccumulateGrad:1676832702136"]
	1676832701912 -> 1676832701744
	1676827509240 [label="model.29.m.0.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676827509240 -> 1676832701912
	1676832701912 [label="AccumulateGrad:1676832701912"]
	1676832701968 -> 1676832701744
	1676827509456 [label="model.29.m.0.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676827509456 -> 1676832701968
	1676832701968 [label="AccumulateGrad:1676832701968"]
	1676832701632 -> 1676832701352
	1676827510464 [label="model.29.m.1.cv1.conv.weight
 (288, 288, 1, 1)" fillcolor=lightblue]
	1676827510464 -> 1676832701632
	1676832701632 [label="AccumulateGrad:1676832701632"]
	1676832701408 -> 1676832701240
	1676827510824 [label="model.29.m.1.cv1.bn.weight
 (288)" fillcolor=lightblue]
	1676827510824 -> 1676832701408
	1676832701408 [label="AccumulateGrad:1676832701408"]
	1676832701464 -> 1676832701240
	1676827511040 [label="model.29.m.1.cv1.bn.bias
 (288)" fillcolor=lightblue]
	1676827511040 -> 1676832701464
	1676832701464 [label="AccumulateGrad:1676832701464"]
	1676832701128 -> 1676832700848
	1676827511904 [label="model.29.m.1.cv2.conv.weight
 (288, 288, 3, 3)" fillcolor=lightblue]
	1676827511904 -> 1676832701128
	1676832701128 [label="AccumulateGrad:1676832701128"]
	1676832700904 -> 1676832700736
	1676827512264 [label="model.29.m.1.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676827512264 -> 1676832700904
	1676832700904 [label="AccumulateGrad:1676832700904"]
	1676832700960 -> 1676832700736
	1676827512480 [label="model.29.m.1.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676827512480 -> 1676832700960
	1676832700960 [label="AccumulateGrad:1676832700960"]
	1676832700624 -> 1676832700400
	1676832700624 [label="SiluBackward:1676832700624" fillcolor=yellow]
	1676832700792 -> 1676832700624
	1676832700792 [label="CudnnBatchNormBackward:1676832700792" fillcolor=yellow]
	1676832725480 -> 1676832700792
	1676832725480 [label="CudnnConvolutionBackward:1676832725480" fillcolor=yellow]
	1676832723632 -> 1676832725480
	1676832725984 -> 1676832725480
	1676827401888 [label="model.29.cv2.conv.weight
 (288, 768, 1, 1)" fillcolor=lightblue]
	1676827401888 -> 1676832725984
	1676832725984 [label="AccumulateGrad:1676832725984"]
	1676832725704 -> 1676832700792
	1676827451464 [label="model.29.cv2.bn.weight
 (288)" fillcolor=lightblue]
	1676827451464 -> 1676832725704
	1676832725704 [label="AccumulateGrad:1676832725704"]
	1676832725872 -> 1676832700792
	1676827451680 [label="model.29.cv2.bn.bias
 (288)" fillcolor=lightblue]
	1676827451680 -> 1676832725872
	1676832725872 [label="AccumulateGrad:1676832725872"]
	1676832700456 -> 1676832700176
	1676827452544 [label="model.29.cv3.conv.weight
 (576, 576, 1, 1)" fillcolor=lightblue]
	1676827452544 -> 1676832700456
	1676832700456 [label="AccumulateGrad:1676832700456"]
	1676832700232 -> 1676832700064
	1676827452904 [label="model.29.cv3.bn.weight
 (576)" fillcolor=lightblue]
	1676827452904 -> 1676832700232
	1676832700232 [label="AccumulateGrad:1676832700232"]
	1676832700288 -> 1676832700064
	1676827453120 [label="model.29.cv3.bn.bias
 (576)" fillcolor=lightblue]
	1676827453120 -> 1676832700288
	1676832700288 [label="AccumulateGrad:1676832700288"]
	1676832699952 -> 1676832699672
	1676827744944 [label="model.33.m.2.weight
 (255, 576, 1, 1)" fillcolor=lightblue]
	1676827744944 -> 1676832699952
	1676832699952 [label="AccumulateGrad:1676832699952"]
	1676832699728 -> 1676832664544
	1676832699728 [label="ViewBackward:1676832699728"]
	1676832700008 -> 1676832699728
	1676827745160 [label="model.33.m.2.bias
 (255)" fillcolor=lightblue]
	1676827745160 -> 1676832700008
	1676832700008 [label="AccumulateGrad:1676832700008"]
	1676832664544 -> 1676827789784
	1676827789856 [label="
 (1, 255, 11, 20)" fillcolor=darkolivegreen1]
	1676832699840 [label="AddBackward0:1676832699840"]
	1676832726712 -> 1676832699840
	1676832726712 [label="CudnnConvolutionBackward:1676832726712" fillcolor=yellow]
	1676832726936 -> 1676832726712
	1676832726936 [label="SiluBackward:1676832726936" fillcolor=yellow]
	1676832743552 -> 1676832726936
	1676832743552 [label="CudnnBatchNormBackward:1676832743552" fillcolor=yellow]
	1676832743664 -> 1676832743552
	1676832743664 [label="CudnnConvolutionBackward:1676832743664" fillcolor=yellow]
	1676832743888 -> 1676832743664
	1676832743888 [label="CatBackward:1676832743888" fillcolor=yellow]
	1676832744056 -> 1676832743888
	1676832744056 [label="SiluBackward:1676832744056" fillcolor=yellow]
	1676832744224 -> 1676832744056
	1676832744224 [label="CudnnBatchNormBackward:1676832744224" fillcolor=yellow]
	1676832744336 -> 1676832744224
	1676832744336 [label="CudnnConvolutionBackward:1676832744336" fillcolor=yellow]
	1676832744560 -> 1676832744336
	1676832744560 [label="SiluBackward:1676832744560" fillcolor=yellow]
	1676832744728 -> 1676832744560
	1676832744728 [label="CudnnBatchNormBackward:1676832744728" fillcolor=yellow]
	1676832744840 -> 1676832744728
	1676832744840 [label="CudnnConvolutionBackward:1676832744840" fillcolor=yellow]
	1676832745064 -> 1676832744840
	1676832745064 [label="SiluBackward:1676832745064" fillcolor=yellow]
	1676832745232 -> 1676832745064
	1676832745232 [label="CudnnBatchNormBackward:1676832745232" fillcolor=yellow]
	1676832745344 -> 1676832745232
	1676832745344 [label="CudnnConvolutionBackward:1676832745344" fillcolor=yellow]
	1676832745568 -> 1676832745344
	1676832745568 [label="SiluBackward:1676832745568" fillcolor=yellow]
	1676832745736 -> 1676832745568
	1676832745736 [label="CudnnBatchNormBackward:1676832745736" fillcolor=yellow]
	1676832745848 -> 1676832745736
	1676832745848 [label="CudnnConvolutionBackward:1676832745848" fillcolor=yellow]
	1676832746072 -> 1676832745848
	1676832746072 [label="SiluBackward:1676832746072" fillcolor=yellow]
	1676832746240 -> 1676832746072
	1676832746240 [label="CudnnBatchNormBackward:1676832746240" fillcolor=yellow]
	1676832746352 -> 1676832746240
	1676832746352 [label="CudnnConvolutionBackward:1676832746352" fillcolor=yellow]
	1676832746576 -> 1676832746352
	1676832746576 [label="CatBackward:1676832746576" fillcolor=yellow]
	1676832746744 -> 1676832746576
	1676832746744 [label="SiluBackward:1676832746744" fillcolor=yellow]
	1676832746856 -> 1676832746744
	1676832746856 [label="CudnnBatchNormBackward:1676832746856" fillcolor=yellow]
	1676832746968 -> 1676832746856
	1676832746968 [label="CudnnConvolutionBackward:1676832746968" fillcolor=yellow]
	1676832699896 -> 1676832746968
	1676832747192 -> 1676832746968
	1676827571040 [label="model.30.conv.weight
 (576, 576, 3, 3)" fillcolor=lightblue]
	1676827571040 -> 1676832747192
	1676832747192 [label="AccumulateGrad:1676832747192"]
	1676832747024 -> 1676832746856
	1676827571400 [label="model.30.bn.weight
 (576)" fillcolor=lightblue]
	1676827571400 -> 1676832747024
	1676832747024 [label="AccumulateGrad:1676832747024"]
	1676832747080 -> 1676832746856
	1676827571616 [label="model.30.bn.bias
 (576)" fillcolor=lightblue]
	1676827571616 -> 1676832747080
	1676832747080 [label="AccumulateGrad:1676832747080"]
	1676824347760 -> 1676832746576
	1676832746632 -> 1676832746352
	1676827572840 [label="model.32.cv1.conv.weight
 (384, 1152, 1, 1)" fillcolor=lightblue]
	1676827572840 -> 1676832746632
	1676832746632 [label="AccumulateGrad:1676832746632"]
	1676832746408 -> 1676832746240
	1676827573200 [label="model.32.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676827573200 -> 1676832746408
	1676832746408 [label="AccumulateGrad:1676832746408"]
	1676832746464 -> 1676832746240
	1676827573416 [label="model.32.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676827573416 -> 1676832746464
	1676832746464 [label="AccumulateGrad:1676832746464"]
	1676832746128 -> 1676832745848
	1676827622424 [label="model.32.m.0.cv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676827622424 -> 1676832746128
	1676832746128 [label="AccumulateGrad:1676832746128"]
	1676832745904 -> 1676832745736
	1676827622784 [label="model.32.m.0.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676827622784 -> 1676832745904
	1676832745904 [label="AccumulateGrad:1676832745904"]
	1676832745960 -> 1676832745736
	1676827623000 [label="model.32.m.0.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676827623000 -> 1676832745960
	1676832745960 [label="AccumulateGrad:1676832745960"]
	1676832745624 -> 1676832745344
	1676827685368 [label="model.32.m.0.cv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	1676827685368 -> 1676832745624
	1676832745624 [label="AccumulateGrad:1676832745624"]
	1676832745400 -> 1676832745232
	1676827685728 [label="model.32.m.0.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676827685728 -> 1676832745400
	1676832745400 [label="AccumulateGrad:1676832745400"]
	1676832745456 -> 1676832745232
	1676827685944 [label="model.32.m.0.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676827685944 -> 1676832745456
	1676832745456 [label="AccumulateGrad:1676832745456"]
	1676832745120 -> 1676832744840
	1676827686952 [label="model.32.m.1.cv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	1676827686952 -> 1676832745120
	1676832745120 [label="AccumulateGrad:1676832745120"]
	1676832744896 -> 1676832744728
	1676827687312 [label="model.32.m.1.cv1.bn.weight
 (384)" fillcolor=lightblue]
	1676827687312 -> 1676832744896
	1676832744896 [label="AccumulateGrad:1676832744896"]
	1676832744952 -> 1676832744728
	1676827687528 [label="model.32.m.1.cv1.bn.bias
 (384)" fillcolor=lightblue]
	1676827687528 -> 1676832744952
	1676832744952 [label="AccumulateGrad:1676832744952"]
	1676832744616 -> 1676832744336
	1676827688392 [label="model.32.m.1.cv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	1676827688392 -> 1676832744616
	1676832744616 [label="AccumulateGrad:1676832744616"]
	1676832744392 -> 1676832744224
	1676827688752 [label="model.32.m.1.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676827688752 -> 1676832744392
	1676832744392 [label="AccumulateGrad:1676832744392"]
	1676832744448 -> 1676832744224
	1676827742280 [label="model.32.m.1.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676827742280 -> 1676832744448
	1676832744448 [label="AccumulateGrad:1676832744448"]
	1676832744112 -> 1676832743888
	1676832744112 [label="SiluBackward:1676832744112" fillcolor=yellow]
	1676832744280 -> 1676832744112
	1676832744280 [label="CudnnBatchNormBackward:1676832744280" fillcolor=yellow]
	1676832756680 -> 1676832744280
	1676832756680 [label="CudnnConvolutionBackward:1676832756680" fillcolor=yellow]
	1676832746576 -> 1676832756680
	1676832757184 -> 1676832756680
	1676827619400 [label="model.32.cv2.conv.weight
 (384, 1152, 1, 1)" fillcolor=lightblue]
	1676827619400 -> 1676832757184
	1676832757184 [label="AccumulateGrad:1676832757184"]
	1676832756904 -> 1676832744280
	1676827619760 [label="model.32.cv2.bn.weight
 (384)" fillcolor=lightblue]
	1676827619760 -> 1676832756904
	1676832756904 [label="AccumulateGrad:1676832756904"]
	1676832757072 -> 1676832744280
	1676827619976 [label="model.32.cv2.bn.bias
 (384)" fillcolor=lightblue]
	1676827619976 -> 1676832757072
	1676832757072 [label="AccumulateGrad:1676832757072"]
	1676832743944 -> 1676832743664
	1676827620840 [label="model.32.cv3.conv.weight
 (768, 768, 1, 1)" fillcolor=lightblue]
	1676827620840 -> 1676832743944
	1676832743944 [label="AccumulateGrad:1676832743944"]
	1676832743720 -> 1676832743552
	1676827621200 [label="model.32.cv3.bn.weight
 (768)" fillcolor=lightblue]
	1676827621200 -> 1676832743720
	1676832743720 [label="AccumulateGrad:1676832743720"]
	1676832743776 -> 1676832743552
	1676827621416 [label="model.32.cv3.bn.bias
 (768)" fillcolor=lightblue]
	1676827621416 -> 1676832743776
	1676832743776 [label="AccumulateGrad:1676832743776"]
	1676832726992 -> 1676832726712
	1676827745520 [label="model.33.m.3.weight
 (255, 768, 1, 1)" fillcolor=lightblue]
	1676827745520 -> 1676832726992
	1676832726992 [label="AccumulateGrad:1676832726992"]
	1676832726768 -> 1676832699840
	1676832726768 [label="ViewBackward:1676832726768"]
	1676832743496 -> 1676832726768
	1676827745736 [label="model.33.m.3.bias
 (255)" fillcolor=lightblue]
	1676827745736 -> 1676832743496
	1676832743496 [label="AccumulateGrad:1676832743496"]
	1676832699840 -> 1676827789856
}
